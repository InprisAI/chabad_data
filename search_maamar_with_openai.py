#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
××•×“×•×œ ×—×™×¤×•×© ××××¨×™× - ×¢×¦×××™ ×•×¤×©×•×˜
====================================

×§×œ×˜:
    maamar_name (str): ×©× ×”××××¨ ×œ×—×™×¤×•×© (×œ××©×œ: "×•××‘×¨×”× ×–×§×Ÿ ×ª×©×œ×´×•")
    question (str, optional): ×©××œ×” ×œ×—×™×¤×•×© × ×•×¡×£ ×‘××™×œ×•×ª ××¤×ª×—

×¤×œ×˜:
    List[Dict]: ×¨×©×™××ª ××××¨×™× ××ª××™××™× ×¢×:
        - name: ×©× ×”××××¨
        - text: ×˜×§×¡×˜ ××œ×
        - filename: ×©× ×”×§×•×‘×¥ (×œ×“×™×‘×•×’)
        - score: ×¦×™×•×Ÿ ×”×ª×××” (0-100)
"""

import pickle
import gzip
import os
import re
from typing import List, Dict, Optional
from dotenv import load_dotenv

# ×˜×¢×Ÿ .env
load_dotenv()

try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False
    print("âš ï¸  Warning: 'requests' not installed. Cannot download from GitHub.")

try:
    from rapidfuzz import fuzz
except ImportError:
    # Fallback to fuzzywuzzy if rapidfuzz not available
    try:
        from fuzzywuzzy import fuzz
    except ImportError:
        print("âŒ Error: Neither 'rapidfuzz' nor 'fuzzywuzzy' installed!")
        raise

try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False
    print("âš ï¸  Warning: 'numpy' not installed. Semantic search unavailable.")

try:
    from openai import OpenAI
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False
    print("âš ï¸  Warning: 'openai' not installed. Semantic search unavailable.")

# ========== GROQ (OpenAI-compatible) CONFIGURATION ==========
# This file is the "2_" version and uses Groq's OpenAI-compatible endpoint.
# Docs: https://console.groq.com/docs/api-reference#chat-create
def _load_kimi_api_key() -> str:
    key = (os.getenv("GROQ_API_KEY") or "").strip()
    if key:
        return key

    # Fallback: repo root (../kimi_code.txt)
    chabad_dir = os.path.dirname(os.path.abspath(__file__))
    repo_root = os.path.dirname(chabad_dir)
    key_path = os.path.join(repo_root, "kimi_code.txt")
    try:
        if os.path.exists(key_path):
            with open(key_path, "r", encoding="utf-8") as f:
                return (f.read() or "").strip()
    except Exception:
        pass

    return ""


KIMI_API_KEY = _load_kimi_api_key()
KIMI_API_BASE_URL = (os.getenv('GROQ_API_BASE_URL') or "").strip() or "https://api.groq.com/openai/v1"
KIMI_CHAT_MODEL = (os.getenv('GROQ_CHAT_MODEL') or "").strip() or "moonshotai/kimi-k2-instruct-0905"
def _build_kimi_chat_completions_url(base_url: str) -> str:
    base = (base_url or "").strip().rstrip("/")
    if base.endswith("/openai/v1") or base.endswith("/v1"):
        return f"{base}/chat/completions"
    return f"{base}/v1/chat/completions"


KIMI_CHAT_COMPLETIONS_URL = _build_kimi_chat_completions_url(KIMI_API_BASE_URL)

# Semantic search uses stored embeddings from the PKL; if those embeddings were created with OpenAI,
# mixing providers will break cosine similarity. ×œ×›×Ÿ ×‘×¨×™×¨×ª ××—×“×œ: ×›×‘×•×™.
ENABLE_SEMANTIC_SEARCH = (os.getenv('ENABLE_SEMANTIC_SEARCH') or "0").strip() in ["1", "true", "True", "yes", "YES"]

# ========== CONFIGURATION ==========
# × ×¡×” ×§×•×“× ××§×•××™, ××—×¨ ×›×š GitHub (×× ×¦×¨×™×š ×œ×©× ×•×ª × ×ª×™×‘)
DEFAULT_PKL_URL = "https://raw.githubusercontent.com/InprisAI/hamara_n/main/chabad/maamarim_unified.pkl.gz"
# IMPORTANT: use a path relative to THIS file (not the current working directory),
# so running the server from repo root still finds the local PKL under `chabad/`.
DEFAULT_LOCAL_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), "maamarim_unified.pkl.gz")
CACHE_PATH = "/tmp/maamarim_cache.pkl.gz"  # Cache for cloud deployments

# ========== GLOBAL CACHE ==========
_MAAMARIM_CACHE = None
_ABBREVIATIONS = {}  # ×˜×‘×œ×ª ×§×™×¦×•×¨×™× ×-__meta__
_INDEX = []          # ××™× ×“×§×¡ ×§×œ ×-__meta__
_KEYWORD_ALIASES = {}  # keyword -> [keyword, aliases...]
_ALIAS_TO_KEYWORD_NORM = {}  # normalize(alias) -> keyword
_EXTRACTED_KEYWORDS_CACHE: Dict[str, List[str]] = {}


# ========== ×¤×•× ×§×¦×™×•×ª ×¢×–×¨ ×œ× ×™×§×•×™ ×§×œ×˜ ==========

def clean_maamar_name(text):
    """
    × ×§×” ××ª ×©× ×”××××¨:
    1. ×”×¡×¨ ×›×œ ××™×œ×” ×©××¡×ª×™×™××ª ×‘-"××××¨" (××××¨, ×‘××××¨, ××”××××¨, ×××××¨ ×•×›×•')
    2. ×”×¡×¨ ×¡×¤×¨×•×ª
    3. ×”×¡×¨ ×¨×•×•×—×™× ××™×•×ª×¨×™×
    """
    if not text:
        return text
    
    # ×”×¡×¨ ×›×œ ××™×œ×” ×©××¡×ª×™×™××ª ×‘-"××××¨" (×¢× ×¨×•×•×— ××—×¨×™×” ××• ×‘×¡×•×£)
    text = re.sub(r'\S*××××¨\s*', '', text.strip())
    
    # ×”×¡×¨ ×¡×¤×¨×•×ª (0-9)
    text = re.sub(r'\d+', '', text)
    
    # ×”×¡×¨ ×¨×•×•×—×™× ××™×•×ª×¨×™×
    text = ' '.join(text.split())
    
    return text


def extract_year_from_text(text):
    """
    ×—×œ×¥ ×©× ×” ×¢×‘×¨×™×ª ××˜×§×¡×˜ (×ª×©...)
    
    ×“×•×’×××•×ª:
    - "×•×™×ª×Ÿ ×œ×š ×ª×©×›×´×—" â†’ ("×•×™×ª×Ÿ ×œ×š", "×ª×©×›×´×—")
    - "×‘××ª×™ ×œ×’× ×™ ×ª×©×™×´×" â†’ ("×‘××ª×™ ×œ×’× ×™", "×ª×©×™×´×")
    - "×–××ª ×—× ×•×›×” ×ª×©×œ"×•" â†’ ("×–××ª ×—× ×•×›×”", "×ª×©×œ×´×•")
    - "××××¨ ×•×™×ª×Ÿ ×œ×š ×ª×©×›×´×— ×¡×”×´×" â†’ ("××××¨ ×•×™×ª×Ÿ ×œ×š ×¡×”×´×", "×ª×©×›×´×—")
    
    Returns:
        tuple: (cleaned_text, year) - ×”×˜×§×¡×˜ ×œ×œ× ×”×©× ×”, ×•×”×©× ×” ×©×—×•×œ×¦×” (××• None)
    """
    if not text:
        return text, None
    
    # ×—×¤×© ×©× ×” ×‘×¤×•×¨××˜: ×ª×© + ××•×ª×™×•×ª ×¢×‘×¨×™×•×ª (×›×•×œ×œ ×’×¨×©×™×™× ×‘×ª×•×š ×”×©× ×”)
    year_pattern = r'×ª×©[×-×ª]{1,2}(?:[×´"×³\'][×-×ª]|[×-×ª])?'
    year_match = re.search(year_pattern, text)
    
    if year_match:
        year_original = year_match.group(0)  # â† ×”×©× ×” ×”××§×•×¨×™×ª ×¢× ×’×¨×©×™×™×
        
        # ×”×¡×¨ ××ª ×”×©× ×” ×”××§×•×¨×™×ª ××”×˜×§×¡×˜
        cleaned_text = text.replace(year_original, '', 1)  # ×”×¡×¨ ×¨×§ ××ª ×”××•×¤×¢ ×”×¨××©×•×Ÿ
        # × ×§×” ×¨×•×•×—×™× ××™×•×ª×¨×™×
        cleaned_text = ' '.join(cleaned_text.split())
        
        # ğŸ†• ×©××•×¨ ××ª ×”×©× ×” ×¢× ×”×’×¨×©×™×™× ×”××§×•×¨×™×™×!
        year = year_original
        
        return cleaned_text, year
    
    return text, None


def parse_complex_input(text):
    """
    ×¤×¨×¡×•×¨ ××•×¨×›×‘ ×©×œ ×˜×§×¡×˜ ×©××›×™×œ: [××¨××” ××§×•×] + [×©× ×”] + [×©××œ×”]
    
    ×“×•×’××”:
    "×‘××××¨ ×•××‘×¨×”× ×–×§×Ÿ ××©× ×ª ×ª×©×œ"×— ×”×©××œ×” ×”×™× ×œ×©× ××” ×”×•×‘× ×”××©×œ ××¨×‘×™ ×–×™×¨×"
    â†’
    {
        'maamar_name': '×•××‘×¨×”× ×–×§×Ÿ',
        'year': '×ª×©×œ×—',
        'question': '×”×©××œ×” ×”×™× ×œ×©× ××” ×”×•×‘× ×”××©×œ ××¨×‘×™ ×–×™×¨×'
    }
    
    Returns:
        dict: {'maamar_name': str, 'year': str or None, 'question': str or None}
    """
    if not text:
        return {'maamar_name': '', 'year': None, 'question': None}
    
    # ×©×œ×‘ 1: × ×§×” ××™×œ×™× ×©××¡×ª×™×™××•×ª ×‘-"××××¨" ×•×¡×¤×¨×•×ª
    text = clean_maamar_name(text)
    
    # ×©×œ×‘ 2: ×—×¤×© ×©× ×” ×¢×‘×¨×™×ª ×‘×˜×§×¡×˜ (×›×•×œ×œ ×›×œ ×¡×•×’×™ ×”×’×¨×©×™×™×)
    year_pattern = r'×ª×©[×-×ª×´"×³\'`â€²â€³â€´]+'
    year_match = re.search(year_pattern, text)
    
    if not year_match:
        # ××™×Ÿ ×©× ×” - ×›×œ ×”×˜×§×¡×˜ ×”×•× ××¨××” ××§×•×
        return {'maamar_name': text.strip(), 'year': None, 'question': None}
    
    year_text = year_match.group(0)
    year_clean = year_text  # ğŸ†• ×©××•×¨ ××ª ×”×©× ×” ×¢× ×”×’×¨×©×™×™× ×”××§×•×¨×™×™×!
    year_start = year_match.start()
    year_end = year_match.end()
    
    # ×©×œ×‘ 3: ×—×œ×§ ××ª ×”×˜×§×¡×˜ ×œ×¤×™ ××™×§×•× ×”×©× ×”
    before_year = text[:year_start].strip()
    after_year = text[year_end:].strip()
    
    # ×©×œ×‘ 4: × ×§×” "××©× ×ª" / "×©× ×ª" / "×‘×©× ×ª" ××”×—×œ×§ ×©×œ×¤× ×™ ×”×©× ×”
    before_year = re.sub(r'\s*(?:××©× ×ª|×?×©× ×ª|×‘×©× ×ª)\s*$', '', before_year).strip()
    
    # ×©×œ×‘ 4.5: ×”×¡×¨ ×¡×¤×¨×•×ª ×××¨××” ××§×•×
    before_year = re.sub(r'\d+', '', before_year).strip()
    
    # ×©×œ×‘ 5: ×× ×™×© ×˜×§×¡×˜ ××—×¨×™ ×”×©× ×” - ×–×• ×”×©××œ×”
    maamar_name = before_year if before_year else None
    question = after_year if after_year else None
    
    return {
        'maamar_name': maamar_name,
        'year': year_clean,
        'question': question
    }


def convert_github_url_to_raw(url: str) -> str:
    """
    ×”××¨ GitHub URL ×¨×’×™×œ ×œ-Raw URL
    
    Example:
        https://github.com/user/repo/blob/main/file.pkl.gz
        â†’
        https://raw.githubusercontent.com/user/repo/main/file.pkl.gz
    """
    if 'github.com' in url and '/blob/' in url:
        url = url.replace('github.com', 'raw.githubusercontent.com')
        url = url.replace('/blob/', '/')
    return url


def download_pkl_from_url(url: str) -> bytes:
    """
    ×”×•×¨×“ PKL ×-URL
    
    Args:
        url: ×›×ª×•×‘×ª ×”×§×•×‘×¥ (GitHub Raw URL ××• HTTP/HTTPS)
    
    Returns:
        bytes: ×ª×•×›×Ÿ ×”×§×•×‘×¥
    """
    if not HAS_REQUESTS:
        raise ImportError("'requests' library required for downloading from URL")
    
    # ×”××¨ ×œ-Raw URL ×× ×¦×¨×™×š
    url = convert_github_url_to_raw(url)
    
    print(f"ğŸ“¥ ××•×¨×™×“ PKL ×-GitHub...")
    print(f"   URL: {url}")
    
    response = requests.get(url, timeout=30)
    
    if response.status_code == 200:
        size_mb = len(response.content) / (1024 * 1024)
        print(f"âœ… ×”×•×¨×“ ×‘×”×¦×œ×—×” ({size_mb:.2f} MB)")
        return response.content
    else:
        raise Exception(f"âŒ ×›×©×œ ×‘×”×•×¨×“×”: HTTP {response.status_code}")


def load_pkl_with_cache(source: Optional[str] = None, use_cache: bool = True) -> Dict:
    """
    ×˜×¢×Ÿ PKL ×¢× cache (×—×›×!)
    
    ××¡×˜×¨×˜×’×™×™×ª ×˜×¢×™× ×”:
    1. ×× ×™×© cache ××§×•××™ ×•×§×™×™× â†’ ×˜×¢×Ÿ ××× ×•
    2. ×× source ×”×•× URL â†’ ×”×•×¨×“ ××”××™× ×˜×¨× ×˜ ×•×©××•×¨ ×‘cache
    3. ×× source ×”×•× × ×ª×™×‘ ××§×•××™ â†’ ×˜×¢×Ÿ ×™×©×™×¨×•×ª
    4. fallback â†’ ×—×¤×© ×‘×ª×™×§×™×™×” ×”× ×•×›×—×™×ª
    
    Args:
        source: URL ××• × ×ª×™×‘ ×œ×§×•×‘×¥ (None = × ×™×¡×™×•×Ÿ ×—×›×)
        use_cache: ×”×× ×œ×”×©×ª××© ×‘cache
    
    Returns:
        Dict: ×”××××¨×™×
    """
    # ×× ×œ× ×¦×•×™×Ÿ source, × ×¡×” ×œ×§×‘×•×¢ ××•×˜×•××˜×™×ª
    if source is None:
        # 1. ×‘×“×•×§ ××©×ª× ×” ×¡×‘×™×‘×”
        source = os.getenv('MAAMARIM_PKL_PATH') or os.getenv('MAAMARIM_PKL_URL')
        
        # 2. ×× ××™×Ÿ - × ×¡×” ×§×•×‘×¥ ××§×•××™
        if source is None:
            if os.path.exists(DEFAULT_LOCAL_PATH):
                source = DEFAULT_LOCAL_PATH
            else:
                # 3. Fallback ×œ-GitHub
                source = DEFAULT_PKL_URL
    
    # ×‘×“×•×§ ×× ×–×” URL
    is_url = source.startswith(('http://', 'https://'))
    
    # ×× ×–×” URL ×•×™×© cache - × ×¡×” cache ×§×•×“×
    if is_url and use_cache and os.path.exists(CACHE_PATH):
        try:
            print(f"ğŸ“‚ ×˜×•×¢×Ÿ ×-cache ××§×•××™...")
            with gzip.open(CACHE_PATH, 'rb') as f:
                return pickle.load(f)
        except Exception as e:
            print(f"âš ï¸  Cache ×¤×’×•×, ××•×¨×™×“ ××—×“×©... ({e})")
    
    # ×˜×¢×Ÿ ××ª ×”×§×•×‘×¥
    if is_url:
        # ×”×•×¨×“ ××”××™× ×˜×¨× ×˜
        content = download_pkl_from_url(source)
        
        # ×©××•×¨ ×œcache
        if use_cache:
            try:
                os.makedirs(os.path.dirname(CACHE_PATH), exist_ok=True)
                with open(CACHE_PATH, 'wb') as f:
                    f.write(content)
                print(f"ğŸ’¾ × ×©××¨ ×œ-cache: {CACHE_PATH}")
            except Exception as e:
                print(f"âš ï¸  ×œ× ×”×¦×œ×—×ª×™ ×œ×©××•×¨ cache: {e}")
        
        # ×˜×¢×Ÿ ××”×–×™×›×¨×•×Ÿ
        return pickle.loads(gzip.decompress(content))
    else:
        # ×˜×¢×Ÿ ××§×•×‘×¥ ××§×•××™
        if not os.path.exists(source):
            raise FileNotFoundError(f"âŒ ×§×•×‘×¥ ×œ× × ××¦×: {source}")
        
        print(f"ğŸ“– ×˜×•×¢×Ÿ ××§×•×‘×¥ ××§×•××™: {source}")
        with gzip.open(source, 'rb') as f:
            return pickle.load(f)


def load_maamarim(source: Optional[str] = None, force_reload: bool = False) -> Dict:
    """
    ×˜×¢×Ÿ ××ª ×”××××¨×™× (×¢× global cache)
    
    Args:
        source: URL ××• × ×ª×™×‘ ×œ×§×•×‘×¥
        force_reload: ×× True, ×˜×¢×Ÿ ××—×“×© ×’× ×× ×›×‘×¨ ×‘×–×™×›×¨×•×Ÿ
    
    Returns:
        Dict: ×”××××¨×™× (×œ×œ× __meta__)
    """
    global _MAAMARIM_CACHE, _ABBREVIATIONS, _INDEX, _KEYWORD_ALIASES, _ALIAS_TO_KEYWORD_NORM
    
    # ×× ×›×‘×¨ ×˜×¢×•×Ÿ ×•×œ× ×“×•×¨×©×™× reload
    if _MAAMARIM_CACHE is not None and not force_reload:
        return _MAAMARIM_CACHE
    
    # ×˜×¢×Ÿ ×—×“×©
    data = load_pkl_with_cache(source)
    
    # ×—×œ×¥ __meta__ ×× ×§×™×™×
    if "__meta__" in data:
        meta = data.pop("__meta__")
        _ABBREVIATIONS = meta.get("abbreviations", {})
        _INDEX = meta.get("index", [])
        _KEYWORD_ALIASES = meta.get("keyword_aliases", {}) or {}
        _ALIAS_TO_KEYWORD_NORM = meta.get("alias_to_keyword_norm", {}) or {}
        print(f"âœ… × ×˜×¢× ×• {len(_ABBREVIATIONS)} ×§×™×¦×•×¨×™× ×•-{len(_INDEX)} ×¨×©×•××•×ª ×‘××™× ×“×§×¡")
    
    _MAAMARIM_CACHE = data
    print(f"âœ… × ×˜×¢× ×• {len(_MAAMARIM_CACHE)} ××××¨×™× ×œ×–×™×›×¨×•×Ÿ")
    
    return _MAAMARIM_CACHE


def get_abbreviations() -> Dict[str, str]:
    """××—×–×™×¨ ××ª ×˜×‘×œ×ª ×”×§×™×¦×•×¨×™×"""
    return _ABBREVIATIONS


def get_index() -> List[Dict]:
    """××—×–×™×¨ ××ª ×”××™× ×“×§×¡ ×”×§×œ"""
    return _INDEX


def normalize_quotes(text: str) -> str:
    """
    × ×¨××œ ××ª ×›×œ ×¡×•×’×™ ×”×’×¨×©×™×™× ×œ×’×¨×© ×× ×’×œ×™ ××—×™×“
    
    ××˜×¤×œ ×‘×ª×•×•×™×:
    - ×³ (U+05F3) - Hebrew geresh
    - ×´ (U+05F4) - Hebrew gershayim
    - ' (U+0027) - ASCII apostrophe
    - " (U+0022) - ASCII quote
    - ` (U+0060) - backtick
    - â€² (U+2032) - prime
    - â€³ (U+2033) - double prime
    """
    if not text:
        return text
    
    # × ×¨××œ ×’×¨×© ×™×—×™×“
    text = text.replace('×³', "'")  # Hebrew geresh â†’ ASCII
    text = text.replace('`', "'")  # backtick â†’ ASCII
    text = text.replace('â€²', "'")  # prime â†’ ASCII
    
    # × ×¨××œ ×’×¨×©×™×™× ×›×¤×•×œ×™×
    text = text.replace('×´', '"')  # Hebrew gershayim â†’ ASCII
    text = text.replace('â€³', '"')  # double prime â†’ ASCII
    
    return text


def expand_abbreviations(text: str) -> str:
    """
    ××¨×—×™×‘ ×§×™×¦×•×¨×™× ×‘×˜×§×¡×˜ ×œ××™×œ×™× ××œ××•×ª
    
    Args:
        text: ×˜×§×¡×˜ ×¢× ×§×™×¦×•×¨×™× (×œ××©×œ: "×©"×¤ ××©×¤×˜×™×")
        
    Returns:
        str: ×˜×§×¡×˜ ××•×¨×—×‘ (×œ××©×œ: "×©×‘×ª ×¤×¨×©×ª ××©×¤×˜×™×")
    """
    if not text or not _ABBREVIATIONS:
        return text
    
    import re
    
    result = text
    
    # ×¢×‘×•×¨ ×›×œ ×§×™×¦×•×¨ ×‘×˜×‘×œ×”, × ×¡×” ×œ××¦×•× ××•×ª×• ×‘×˜×§×¡×˜ ×¢× ×›×œ ×¡×•×’×™ ×”×’×¨×©×™×™×
    for abbr_original, meaning in _ABBREVIATIONS.items():
        if not abbr_original or not meaning:
            continue
        
        # × ×¡×” ××ª ×”×§×™×¦×•×¨ ×”××§×•×¨×™ (×¢× ×”×’×¨×©×™×™× ×©×œ×•)
        if abbr_original in result:
            result = result.replace(abbr_original, meaning)
            continue
        
        # × ×¡×” ×¢× ×›×œ ×¡×•×’×™ ×”×’×¨×©×™×™× ×”××¤×©×¨×™×™×
        # ×›×œ ×¡×•×’×™ ×”×’×¨×©×™×™×: ×´ " ×³ ' ` â€² â€³ â€´
        quote_chars = ['"', "'", '×´', '×³', '`', 'â€²', 'â€³', 'â€´']
        
        # ×× ×”×§×™×¦×•×¨ ××›×™×œ ×’×¨×©×™×™×, × ×¡×” ×›×œ ×©×™×œ×•×‘
        if '"' in abbr_original or "'" in abbr_original or '×´' in abbr_original or '×³' in abbr_original:
            # ×‘× ×” variants ×¢× ×›×œ ×¡×•×’×™ ×”×’×¨×©×™×™×
            for q in quote_chars:
                # ×”×—×œ×£ ××ª ×”×’×¨×©×™×™× ×‘×§×™×¦×•×¨ ×”××§×•×¨×™
                abbr_variant = abbr_original.replace('"', q).replace("'", q).replace('×´', q).replace('×³', q)
                if abbr_variant in result:
                    result = result.replace(abbr_variant, meaning)
                    break
        else:
            # ××™×Ÿ ×’×¨×©×™×™× - ×¤×©×•×˜ × ×¡×” ××ª ×”×§×™×¦×•×¨ ×”××§×•×¨×™
            if abbr_original in result:
                result = result.replace(abbr_original, meaning)
    
    return result


def normalize_text(text: str, level: int = 0) -> str:
    """
    × ×¨××œ ×˜×§×¡×˜ ×œ×”×©×•×•××” (×”×¡×¨ × ×™×§×•×“, ×¨×•×•×—×™× ××™×•×ª×¨×™×, ×•×›×•')
    
    Args:
        text: ×”×˜×§×¡×˜ ×œ× ×¨××•×œ
        level: ×¨××ª × ×•×¨××œ×™×–×¦×™×”:
               0 = ×‘×¡×™×¡×™ (×¨×§ × ×™×§×•×“ ×•×’×¨×©×™×™×)
               1 = + ×”×¡×¨ ×•×³ ×××¦×¢×™×•×ª
               2 = + ×”×¡×¨ ×’× ×™×³ ×××¦×¢×™×•×ª
               3 = + ×”×¡×¨ ×’× ×”×³ ×××¦×¢×™×•×ª (×™×”×•× ×ª×Ÿ â†’ ×™×•× ×ª×Ÿ)
    """
    if not text:
        return ""
    
    # ×”×¡×¨ × ×™×§×•×“ ×¢×‘×¨×™ (U+0591 - U+05C7)
    text = re.sub(r'[\u0591-\u05C7]', '', text)
    
    # ğŸ†• ×”×¡×¨ ×’×¨×©×™×™× ×œ×œ× ×”×—×œ×¤×” ×‘×¨×•×•×— (×›×“×™ ×œ×©××•×¨ ×¢×œ ×¨××©×™ ×ª×™×‘×•×ª)
    text = re.sub(r'[×´"×³\'`â€²â€³â€´]', '', text)
    
    # ×¨××ª × ×•×¨××œ×™×–×¦×™×” 1: ×”×¡×¨ ×•×³ ×××¦×¢×™×•×ª
    if level >= 1:
        text = re.sub(r'([×-×ª])×•([×-×ª])', r'\1\2', text)
    
    # ×¨××ª × ×•×¨××œ×™×–×¦×™×” 2: ×”×¡×¨ ×’× ×™×³ ×××¦×¢×™×•×ª
    if level >= 2:
        text = re.sub(r'([×-×ª])×™([×-×ª])', r'\1\2', text)
    
    # ×¨××ª × ×•×¨××œ×™×–×¦×™×” 3: ×”×¡×¨ ×’× ×”×³ ×××¦×¢×™×•×ª (×™×”×•× ×ª×Ÿ â†’ ×™×•× ×ª×Ÿ)
    if level >= 3:
        text = re.sub(r'([×-×ª])×”([×-×ª])', r'\1\2', text)
    
    # ×”×¡×¨ ×¡×™×× ×™ ×¤×™×¡×•×§
    text = re.sub(r'[,.\-:;!?()[\]{}]', ' ', text)
    
    # ×”×¡×¨ ×¨×•×•×—×™× ××™×•×ª×¨×™×
    text = ' '.join(text.split())
    
    return text.strip()


def extract_keywords_from_question(question: str) -> Optional[List[str]]:
    """
    ××©×ª××© ×‘-OpenAI ×œ×—×™×œ×•×¥ ××™×œ×•×ª ××¤×ª×— ×—×©×•×‘×•×ª ××”×©××œ×”
    
    Args:
        question: ×”×©××œ×” ×©×œ ×”××©×ª××©
    
    Returns:
        ×¨×©×™××ª ××™×œ×•×ª ××¤×ª×—, ××• None ×× × ×›×©×œ
    """
    q_key = (question or "").strip()
    if q_key and q_key in _EXTRACTED_KEYWORDS_CACHE:
        return _EXTRACTED_KEYWORDS_CACHE[q_key]

    if not HAS_REQUESTS:
        print("âš ï¸  requests ×œ× ××•×ª×§×Ÿ")
        return None
        
    if not KIMI_API_KEY:
        print("âš ï¸  GROQ_API_KEY ×œ× ××•×’×“×¨ - ××©×ª××© ×‘-fallback")
        return None
    
    prompt = f"""××ª×” ×¢×•×–×¨ ×œ×—×œ×¥ ××™×œ×•×ª ××¤×ª×— ××—×™×¤×•×©×™×.

×§×‘×œ ××ª ×”×©××œ×” ×”×‘××” ×•×—×œ×¥ **××ª ×›×œ ××™×œ×•×ª ×”××¤×ª×— ×”×—×©×•×‘×•×ª** ×œ×—×™×¤×•×© ×‘××××¨×™ ×—×¡×™×“×•×ª.

**×”×•×¨××•×ª ×§×¨×™×˜×™×•×ª:**
1. ×—×œ×¥ **××ª ×›×œ** ×©××•×ª ×©×œ ×× ×©×™×, ××•×©×’×™×, ××§×•××•×ª, ×—×’×™×, ××¦×•×•×ª ×©× ××¦××™× ×‘×©××œ×”
2. **××œ ×ª×›×œ×•×œ** ××™×œ×•×ª ×¢×–×¨ ×›××•: ××”, ××™, ××™×š, ×œ××”, ×”×¨×‘, ×“×¢×ª, ××•××¨, ×¢×œ, ×©×œ, ××ª, ×¢×, ×œ×¤×™, ×”×™×, ×”×•×, ×–×”, ×–×•
3. ×”×—×–×¨ **××ª ×›×œ** ××™×œ×•×ª ×”××¤×ª×— ×”×—×©×•×‘×•×ª ×©× ××¦××•×ª ×‘×©××œ×” (××™×Ÿ ×”×’×‘×œ×” ×¢×œ ××¡×¤×¨ ×”××™×œ×•×ª)
4. ×× ××™×Ÿ ××™×œ×•×ª ××¤×ª×— ×—×©×•×‘×•×ª - ×›×ª×•×‘ "××™×Ÿ"
5. **×—×©×•×‘:** ×× ×”×©××œ×” ××›×™×œ×” ××¡×¤×¨ ××™×œ×•×ª ××¤×ª×—, ×”×—×–×¨ ××ª ×›×•×œ×Ÿ - ××œ ×ª×“×œ×’ ×¢×œ ××£ ××—×ª
6. **××¡×•×¨ ×œ×™×¦×•×¨ ×¨××©×™ ×ª×™×‘×•×ª!** ×”×—×–×¨ ×¨×§ ××™×œ×•×ª ××¤×ª×— ××œ××•×ª (3+ ××•×ª×™×•×ª). ×× ×”××©×ª××© ×›×ª×‘ ×¨××©×™ ×ª×™×‘×•×ª ×¢× ×’×¨×©×™×™× (×›××• "×¡×˜"×") - ××¤×©×¨ ×œ×”×—×–×™×¨ ××•×ª×. ××‘×œ ××¡×•×¨ ×œ×™×¦×•×¨ ×¨××©×™ ×ª×™×‘×•×ª ×—×“×©×™×!

**âš ï¸ ×”×•×¨××” ×§×¨×™×˜×™×ª - ×©××™×¨×” ×¢×œ ×¦×™×¨×•×¤×™ ××™×œ×™×:**
- **××œ ×ª×¤×¨×§ ×¦×™×¨×•×¤×™ ××™×œ×™×!** ×©××•×¨ ×¢×œ×™×”× ×›××™×œ×ª ××¤×ª×— ××—×ª
- "×¡×™×˜×¨× ××—×¨×" = ××™×œ×ª ××¤×ª×— ××—×ª (âŒ ×œ× "×¡×™×˜×¨×, ××—×¨×")
- "××—×ª ×¢×©×¨×”" = ××™×œ×ª ××¤×ª×— ××—×ª (âŒ ×œ× "××—×ª, ×¢×©×¨×”")
- "×‘×¨×™××ª ×”×¢×•×œ×" = ××™×œ×ª ××¤×ª×— ××—×ª
- ×× ×™×© ×¦×™×¨×•×£ ×©×œ ××¡×¤×¨ + ×©× (×›××• "××—×ª ×¢×©×¨×” ×‘×—×™× ×•×ª"), ×”×—×–×¨ ××ª ×”×¦×™×¨×•×£ ×”××œ× ×›××™×œ×ª ××¤×ª×— ××—×ª
- **×—×©×•×‘:** ×× ×”×©××œ×” ××›×™×œ×” "×¡×™×˜×¨× ××—×¨×" - ×”×—×–×¨ "×¡×™×˜×¨× ××—×¨×" (2 ××™×œ×™× ×‘×™×—×“), ×œ× "×¡×™×˜×¨×" ×•"××—×¨×" × ×¤×¨×“×™×
- **×—×©×•×‘:** ×× ×”×©××œ×” ××›×™×œ×” "××—×ª ×¢×©×¨×”" - ×”×—×–×¨ "××—×ª ×¢×©×¨×”" (2 ××™×œ×™× ×‘×™×—×“), ×œ× "××—×ª" ×•"×¢×©×¨×”" × ×¤×¨×“×™×

**ğŸš« ××¡×•×¨ ×œ×”×•×¡×™×£ ×’×¨×©×™×™× ×œ××™×œ×™×!**
- ×× ×”××©×ª××© ×›×ª×‘ "×ª×©×›×‘" (×œ×œ× ×’×¨×©×™×™×) - **××¡×•×¨** ×œ×”×—×–×™×¨ "×ª×©×›"×‘" (×¢× ×’×¨×©×™×™×)!
- ×¨×§ ×× ×”××©×ª××© ×›×ª×‘ **×‘×¢×¦××•** ××™×œ×” ×¢× ×’×¨×©×™×™× (×›××• "×¡×˜"×") - ×ª×—×–×™×¨ ××•×ª×” ×›××• ×©×”×™×
- ××—×¨×ª - ×”×—×–×¨ ××ª ×”××™×œ×” **×‘×“×™×•×§ ×›×¤×™ ×©×”××©×ª××© ×›×ª×‘** (×œ×œ× ×’×¨×©×™×™×)

**×“×•×’×××•×ª (×©×™××• ×œ×‘ - ×¦×™×¨×•×¤×™ ××™×œ×™× × ×©××¨×™× ×™×—×“!):**
- "××” ×“×¢×ª ×”×¨×‘ ×¢×œ ×“×•×“ ×•×™×”×•× ×ª×Ÿ" â†’ ×“×•×“, ×™×”×•× ×ª×Ÿ
- "××™×š ×”××“××•×¨ ××¡×‘×™×¨ ××ª ×¢× ×™×Ÿ ×”×’××•×œ×”" â†’ ××“××•×¨, ×’××•×œ×”
- "××” ×”×§×©×¨ ×‘×™×Ÿ ×©×‘×ª ×œ×‘×¨×™××ª ×”×¢×•×œ×" â†’ ×©×‘×ª, ×‘×¨×™××ª ×”×¢×•×œ×
- "××” ×–×” ×¡×¤×™×¨×•×ª" â†’ ×¡×¤×™×¨×•×ª
- "××” ×–×” ×¡×™×˜×¨× ××—×¨× ×‘×—×™× ×•×ª ×“×•×“ ×•×™×”×•× ×ª×Ÿ" â†’ ×¡×™×˜×¨× ××—×¨×, ×‘×—×™× ×•×ª, ×“×•×“, ×™×”×•× ×ª×Ÿ
- "××” ×–×” ×¡×™×˜×¨× ××—×¨× ××—×ª ×¢×©×¨×” ×‘×—×™× ×•×ª" â†’ ×¡×™×˜×¨× ××—×¨×, ××—×ª ×¢×©×¨×”, ×‘×—×™× ×•×ª

**×”×©××œ×”:**
{question}

**××™×œ×•×ª ××¤×ª×— (××•×¤×¨×“×•×ª ×‘×¤×¡×™×§×™×, ×”×—×–×¨ ××ª ×›×œ ×”××™×œ×•×ª ×”×—×©×•×‘×•×ª ×•×©××•×¨ ×¢×œ ×¦×™×¨×•×¤×™ ××™×œ×™× ×›××™×œ×” ××—×ª - ××œ ×ª×¤×¨×§ ×¦×™×¨×•×¤×™ ××™×œ×™×!):"""

    try:
        response = requests.post(
            KIMI_CHAT_COMPLETIONS_URL,
            headers={
                "Authorization": f"Bearer {KIMI_API_KEY}",
                "Content-Type": "application/json"
            },
            json={
                "model": KIMI_CHAT_MODEL,
                "messages": [{"role": "user", "content": prompt}],
                # Use temperature 0 for repeatability/determinism
                "temperature": 0.0,
                "max_tokens": 1000
            },
            timeout=10
        )
        
        if response.status_code == 200:
            answer = response.json()['choices'][0]['message']['content'].strip()
            
            if answer.lower() == '××™×Ÿ':
                _EXTRACTED_KEYWORDS_CACHE[q_key] = []
                return []
            
            # ×¤×¨×§ ×œ××™×œ×•×ª ××¤×ª×— (×ª××™×“ ×œ×¤×™ ×¤×¡×™×§×™×, ××‘×œ ×’× ×ª××•×š ×‘×¤×¡×™×§×™× ×¢× ×¨×•×•×—×™×)
            # ×”×¡×¨ ×¨×•×•×—×™× ××™×•×ª×¨×™× ×œ×¤× ×™ ×•××—×¨×™ ×›×œ ××™×œ×ª ××¤×ª×—
            keywords = [kw.strip() for kw in answer.split(',') if kw.strip()]
            
            # ×× ×œ× × ××¦××• ××™×œ×•×ª ××¤×ª×— (××•×œ×™ Kimi ×”×—×–×™×¨ ××©×”×• ××—×¨), × ×¡×” ×œ×¤×¨×§ ×œ×¤×™ ×©×•×¨×•×ª
            if not keywords:
                keywords = [kw.strip() for kw in answer.split('\n') if kw.strip() and kw.strip() != '××™×Ÿ']
            
            # ğŸ†• ×× ×™×© "×•" ×‘×™×Ÿ ××™×œ×•×ª ××¤×ª×—, ×¤×¨×§ ×’× ×œ×¤×™ "×•" (×œ××©×œ: "×“×•×“ ×•×™×”×•× ×ª×Ÿ" â†’ ["×“×•×“", "×™×”×•× ×ª×Ÿ"])
            if keywords:
                expanded = []
                for kw in keywords:
                    # ×× ×™×© "×•" ×‘××™×œ×ª ××¤×ª×—, ×¤×¨×§ ××•×ª×”
                    if ' ×•' in kw:
                        # ×¤×¨×§ ×œ×¤×™ " ×•" (×¨×•×•×— + ×•)
                        parts = kw.split(' ×•')
                        for p in parts:
                            p_clean = p.strip()
                            if p_clean:
                                expanded.append(p_clean)
                    elif kw.startswith('×•') and len(kw) > 1:
                        # ×× ××ª×—×™×œ ×‘-"×•", ×”×¡×¨ ××•×ª×• ×•×”×•×¡×£ ××ª ×”××™×œ×”
                        expanded.append(kw[1:].strip())
                    else:
                        expanded.append(kw)
                keywords = expanded
            
            # ğŸ†• ×”×¡×¨ ××¡×¤×¨×™× ×××™×œ×•×ª ××¤×ª×— (×× ×™×©)
            keywords_cleaned = []
            for kw in keywords:
                kw_cleaned = _remove_numbers_from_keyword(kw)
                # ×× ×”×’×¨×¡×” ×œ×œ× ××¡×¤×¨ ×œ× ×¨×™×§×” ×•×©×•× ×” ××”××§×•×¨×™×ª, ×”×©×ª××© ×‘×”
                if kw_cleaned and kw_cleaned.strip() and kw_cleaned != kw:
                    keywords_cleaned.append(kw_cleaned)
                else:
                    keywords_cleaned.append(kw)
            
            keywords = keywords_cleaned
            
            # ğŸ†• ××™×™×Ÿ ×œ×¤×™ ×"×‘
            keywords.sort()
            
            # ğŸš« ×¡×™× ×•×Ÿ: ×”×¡×¨ ××™×œ×•×ª ××¤×ª×— ×§×¦×¨×•×ª ××“×™ ×©××™× ×Ÿ ×¨××©×™ ×ª×™×‘×•×ª ×œ×’×™×˜×™××™×™×
            # ×× ××™×œ×” ×”×™× 1-2 ××•×ª×™×•×ª ×‘×œ×‘×“ (×œ×œ× ×’×¨×©×™×™×) - ×–×• ×›× ×¨××” ×˜×¢×•×ª!
            keywords_filtered = []
            for kw in keywords:
                kw_no_quotes = re.sub(r'[×´"×³\'`â€²â€³â€´]', '', kw).strip()
                # ×× ×™×© ×’×¨×©×™×™× ×‘××™×œ×” ×”××§×•×¨×™×ª - ×–×” ×¨××©×™ ×ª×™×‘×•×ª ×œ×’×™×˜×™××™, ×©××•×¨ ××•×ª×•
                has_quotes = any(ch in kw for ch in ['×´', '"', '×³', "'", '`', 'â€²', 'â€³', 'â€´'])
                # ×× ×”××™×œ×” ××¨×•×›×” ××¡×¤×™×§ (3+ ××•×ª×™×•×ª) ××• ×©×™×© ×‘×” ×’×¨×©×™×™× - ×©××•×¨ ××•×ª×”
                if len(kw_no_quotes) >= 3 or has_quotes:
                    keywords_filtered.append(kw)
                else:
                    print(f"   âš ï¸ ××¡×™×¨ ××™×œ×” ×§×¦×¨×” ××“×™ ×œ×œ× ×’×¨×©×™×™×: '{kw}'")
            
            keywords = keywords_filtered
            
            print(f"ğŸ¤– Groq ×—×™×œ×¥ ××™×œ×•×ª ××¤×ª×—: {keywords}")
            if q_key:
                _EXTRACTED_KEYWORDS_CACHE[q_key] = keywords
            return keywords
        else:
            return None
            
    except Exception as e:
        print(f"âš ï¸  ×©×’×™××” ×‘×—×™×œ×•×¥ ××™×œ×•×ª ××¤×ª×—: {e}")
        return None


def extract_maamar_name_only(text: str) -> str:
    """
    ×—×œ×¥ ××ª ×©× ×”××××¨ ×”×××™×ª×™ ××”×˜×§×¡×˜ (×œ×œ× ×©× ×” ×•××¨××” ××§×•×)
    
    ×œ×•×’×™×§×”:
    1. ×”×¡×¨ ××ª ×”×©× ×” (××œ×” ×©××ª×—×™×œ×” ×‘-"×ª×©" ×•××›×™×œ×” ×’×¨×©×™×™×)
    2. ×§×— ×¨×§ ×¢×“ ×”××œ×” ×”×¨××©×•× ×” ×¢× ×’×¨×©×™×™× (××¨××” ××§×•×)
    
    ×“×•×’×××•×ª:
    - "×•×™×ª×Ÿ ×œ×š ×ª×©×›×´×— ×¡×”×´× ××œ×•×§×˜ ×“" â†’ "×•×™×ª×Ÿ ×œ×š"
    - "×‘××ª×™ ×œ×’× ×™ ×ª×©×™×´×" â†’ "×‘××ª×™ ×œ×’× ×™"
    - "×•×™×ª×Ÿ ×œ×š ×¡×”×´× ××œ×•×§×˜" â†’ "×•×™×ª×Ÿ ×œ×š"
    
    Args:
        text: ×”×˜×§×¡×˜ ×”××œ× ×©×”×•×§×©
    
    Returns:
        str: ×©× ×”××××¨ ×‘×œ×‘×“ (×œ×œ× ×©× ×” ×•××¨××” ××§×•×)
    """
    if not text:
        return text
    
    # ğŸ†• ×”×¡×¨ "×“'×”" ××• "×“"×”" ×•×›×•' ××”×˜×§×¡×˜ (×¢× ××• ×‘×œ×™ ×¤×™×™×¤×™×)
    text = re.sub(r'\|?×“[×´"×³\'`â€²â€³â€´]×”\|?\s*', '', text).strip()
    
    words = text.split()
    result_words = []
    
    for word in words:
        # ×‘×“×•×§ ×× ×”××œ×” ××›×™×œ×” ×’×¨×©×™×™× (×´ ××• ×³)
        if '×´' in word or '×³' in word:
            # ×× ×–×• ××œ×ª ×©× ×” (××ª×—×™×œ ×‘-"×ª×©") - ×“×œ×’ ×¢×œ×™×”!
            if word.startswith('×ª×©'):
                continue  # ×“×œ×’ ×¢×œ ×”×©× ×”!
            # ××¦×× ×• ××œ×” ××—×¨×ª ×¢× ×’×¨×©×™×™× (××¨××” ××§×•×) - ×¢×¦×•×¨ ×›××Ÿ
            break
        else:
            result_words.append(word)
    
    return ' '.join(result_words).strip()


def exact_search_name(maamar_name: str, maamarim: Dict, top_n: int = 10) -> List[Dict]:
    """
    ×—×™×¤×•×© ××“×•×™×§ ×œ×¤×™ ×©× ×”××××¨ - ×¨×§ ×”×ª×××•×ª ××“×•×™×§×•×ª, ×œ×œ× fuzzy/×§×™×¦×•×¨×™×
    
    Args:
        maamar_name: ×©× ×”××××¨ ×œ×—×™×¤×•×© (×œ×œ× ×’×¨×©×™×™×)
        maamarim: ×”××××¨×™× ×”×˜×¢×•× ×™×
        top_n: ××¡×¤×¨ ×ª×•×¦××•×ª ××§×¡×™××œ×™
    
    Returns:
        List[Dict]: ×¨×©×™××ª ××××¨×™× ×©××›×™×œ×™× ××ª ×”××™×œ×” ×”××“×•×™×§×ª
    """
    results = []
    
    # × ×¨××œ ××ª ×”×©××™×œ×ª×” ×‘×¨××” ×‘×¡×™×¡×™×ª ×‘×œ×‘×“ (×œ×œ× ×”×¨×—×‘×•×ª ×§×™×¦×•×¨×™×)
    query_normalized = normalize_text(maamar_name, level=0)
    
    if not query_normalized.strip():
        return []
    
    print(f"ğŸ¯ ×—×™×¤×•×© ××“×•×™×§: '{query_normalized}'")
    
    for key, maamar in maamarim.items():
        name = maamar.get('name', '')
        if not name:
            continue
        
        # × ×¨××œ ××ª ×©× ×”××××¨ ×‘×¨××” ×‘×¡×™×¡×™×ª
        name_normalized = normalize_text(name, level=0)
        
        # ×‘×“×•×§ ×× ×”×©××™×œ×ª×” ×§×™×™××ª ×›××™×œ×” ×©×œ××” ×‘×©× ×”××××¨
        # ×”×©×ª××© ×‘-word boundaries ×›×“×™ ×œ×× ×•×¢ ×”×ª×××” ×—×œ×§×™×ª
        pattern = r'\b' + re.escape(query_normalized) + r'\b'
        
        if re.search(pattern, name_normalized):
            results.append({
                'key': key,
                'name': name,
                'year': maamar.get('year', ''),
                'filename': maamar.get('filename', ''),
                'text': maamar.get('text', ''),
                'keywords_all': maamar.get('keywords_all', []),
                'embedding': maamar.get('embedding'),
                'score': 100,  # ×”×ª×××” ××“×•×™×§×ª ×ª××™×“ 100
                'fuzzy_score': 100,
                'keyword_score': 0,
                'semantic_score': 0
            })
    
    # ××™×™×Ÿ ×œ×¤×™ ××•×¨×š ×”×©× (×›×›×œ ×©×”×©× ×§×¦×¨ ×™×•×ª×¨, ×›× ×¨××” ×¨×œ×•×•× ×˜×™ ×™×•×ª×¨)
    results.sort(key=lambda x: len(x['name']))
    
    print(f"   âœ… × ××¦××• {len(results)} ×”×ª×××•×ª ××“×•×™×§×•×ª")
    return results[:top_n]


def fuzzy_search_name(maamar_name: str, maamarim: Dict, top_n: int = 10) -> List[Dict]:
    """
    ×—×™×¤×•×© fuzzy ×œ×¤×™ ×©× ×”××××¨
    
    ×œ×•×’×™×§×” ×—×“×©×”:
    1. ×›×œ ×”××œ×™× ×‘×¡×“×¨ × ×›×•×Ÿ = 100%
    2. ××œ×” ×¨××©×•× ×” ×ª×•×××ª = ×”×ª×—×œ ×‘-100%
    3. ××œ×” ×¨××©×•× ×” ×œ× ×ª×•×××ª ××‘×œ ×§×™×™××ª = ×”×ª×—×œ ×‘-90%
    4. ××œ×” ×¨××©×•× ×” ×œ× ×§×™×™××ª = 0
    
    ×§× ×¡×•×ª ×œ××œ×™× × ×•×¡×¤×•×ª:
    - ××œ×” ×œ× ×§×™×™××ª ×‘××××¨ = -15 × ×§×•×“×•×ª
    - ××œ×” ×§×™×™××ª ××‘×œ ×œ× ×‘××§×•× = -5 × ×§×•×“×•×ª
    
    ×“×•×’×××•×ª:
    - "×•×™×××¨ ×™×”×•× ×ª×Ÿ" ×‘××××¨ "×•×™×××¨ ×œ×• ×™×”×•× ×ª×Ÿ" â†’ 95% (×¨××©×•× ×” ×ª×•×××ª, ×©× ×™×™×” ×œ× ×‘××§×•×: 100-5)
    - "×•×™×××¨ ×™×”×•× ×ª×Ÿ" ×‘××××¨ "×•×™×××¨ ×”×³ ××œ ××‘×¨×" â†’ 85% (×¨××©×•× ×” ×ª×•×××ª, ×©× ×™×™×” ×œ× ×§×™×™××ª: 100-15)
    - "×™×”×•× ×ª×Ÿ ×•×™×××¨" ×‘××××¨ "×•×™×××¨ ×œ×• ×™×”×•× ×ª×Ÿ" â†’ 85% (×¨××©×•× ×” ×§×™×™××ª, ×©× ×™×™×” ×œ× ×‘××§×•×: 90-5)
    
    Args:
        maamar_name: ×©× ×”××××¨ ×œ×—×™×¤×•×©
        maamarim: ×”××××¨×™× ×”×˜×¢×•× ×™×
        top_n: ××¡×¤×¨ ×ª×•×¦××•×ª ××§×¡×™××œ×™
    
    Returns:
        List[Dict]: ×¨×©×™××ª ××××¨×™× ×¢× ×¦×™×•×Ÿ ×”×ª×××”
    """
    results = []
    
    # ğŸ†• ×”×¨×—×‘ ×§×™×¦×•×¨×™× ×‘×©××™×œ×ª×” ×œ×¤× ×™ × ×•×¨××œ×™×–×¦×™×”
    maamar_name_expanded = expand_abbreviations(maamar_name)
    
    # ğŸ†• × ×¨××œ ××ª ×”×©××™×œ×ª×” ×‘×›×œ ×”×¨××•×ª
    query_level0 = normalize_text(maamar_name_expanded, level=0)
    query_level1 = normalize_text(maamar_name_expanded, level=1)
    query_level2 = normalize_text(maamar_name_expanded, level=2)
    
    query_words = query_level0.split()
    
    if not query_words:
        return []
    
    for key, maamar in maamarim.items():
        name = maamar.get('name', '')
        if not name:
            continue
        
        # ğŸ†• ×”×¨×—×‘ ×§×™×¦×•×¨×™× ×‘×©× ×”××××¨
        name_expanded = expand_abbreviations(name)
        
        # ğŸ†• × ×¨××œ ××ª ×©× ×”××××¨ ×‘×›×œ ×”×¨××•×ª
        name_level0 = normalize_text(name_expanded, level=0)
        name_level1 = normalize_text(name_expanded, level=1)
        name_level2 = normalize_text(name_expanded, level=2)
        
        name_words_level0 = name_level0.split()
        name_words_level1 = name_level1.split()
        name_words_level2 = name_level2.split()
        
        if not name_words_level0:
            continue
        
        # ğŸ†• ×—×™×©×•×‘ ×¦×™×•×Ÿ ×‘-3 ×¡×‘×‘×™×
        words_found_count = 0
        words_in_order = 0
        total_penalty = 0  # ×¡×”"×› ×§× ×¡ ×¢×œ × ×•×¨××œ×™×–×¦×™×”
        
        query_words_level1 = query_level1.split()
        query_words_level2 = query_level2.split()
        
        for i, query_word_l0 in enumerate(query_words):
            query_word_l1 = query_words_level1[i] if i < len(query_words_level1) else query_word_l0
            query_word_l2 = query_words_level2[i] if i < len(query_words_level2) else query_word_l0
            
            found = False
            penalty = 0
            
            # ×¡×‘×‘ 1: × ×¡×” ×”×ª×××” ××“×•×™×§×ª (level 0)
            for j, name_word in enumerate(name_words_level0):
                if query_word_l0 == name_word:
                    words_found_count += 1
                    if i == j:
                        words_in_order += 1
                    found = True
                    penalty = 0  # ××™×Ÿ ×§× ×¡
                    break
            
            # ×¡×‘×‘ 2: ×× ×œ× × ××¦×, × ×¡×” ×¢× × ×•×¨××•×œ ×•×³ (level 1)
            if not found and query_word_l1 != query_word_l0:
                for j, name_word in enumerate(name_words_level1):
                    if query_word_l1 == name_word:
                        words_found_count += 1
                        if i == j:
                            words_in_order += 1
                        found = True
                        penalty = 5  # ×§× ×¡ 5 × ×§×•×“×•×ª
                        break
            
            # ×¡×‘×‘ 3: ×× ×¢×“×™×™×Ÿ ×œ× × ××¦×, × ×¡×” ×¢× × ×•×¨××•×œ ×™×³ (level 2)
            if not found and query_word_l2 != query_word_l1:
                for j, name_word in enumerate(name_words_level2):
                    if query_word_l2 == name_word:
                        words_found_count += 1
                        if i == j:
                            words_in_order += 1
                        found = True
                        penalty = 5  # ×§× ×¡ 5 × ×§×•×“×•×ª × ×•×¡×¤×•×ª (×¡×”"×› 10)
                        break
            
            total_penalty += penalty
        
        # ×× ××£ ××œ×” ×œ× × ××¦××” - ×“×œ×’ ×¢×œ ×”××××¨
        if words_found_count == 0:
            continue
        
        # ×¦×™×•×Ÿ ×‘×¡×™×¡×™ = (××¡×¤×¨ ××œ×™× ×©× ××¦××• / ×›×œ ×”××œ×™× ×‘×—×™×¤×•×©) Ã— 100
        base_score = int((words_found_count / len(query_words)) * 100)
        
        # ×‘×•× ×•×¡ ×œ×¡×“×¨: +10 ×œ×›×œ ××œ×” ×‘××§×•× ×”× ×›×•×Ÿ
        order_bonus = int((words_in_order / len(query_words)) * 10)
        
        # ×‘×•× ×•×¡ +10 ×× ×”××œ×” ×”×¨××©×•× ×” ×ª×•×××ª ×‘××§×•× ×”×¨××©×•×Ÿ
        first_word_bonus = 10 if (len(name_words_level0) > 0 and query_words[0] == name_words_level0[0]) else 0
        
        # ğŸ†• ×”×¤×—×ª ×§× ×¡ × ×•×¨××œ×™×–×¦×™×”
        score = base_score + order_bonus + first_word_bonus - total_penalty
        
        # ×”×’×‘×œ ×¦×™×•×Ÿ (××™× ×™××•× 0, ××§×¡×™××•× 100)
        score = max(0, min(100, score))
        
        # ×”×•×¡×£ ×œ×ª×•×¦××•×ª
        if score > 0:
            results.append({
                'key': key,
                'name': name,
                'filename': maamar.get('filename', ''),
                'text': maamar.get('text', ''),
                'year': maamar.get('year'),  # ğŸ†• ×©× ×” ××”-PKL
                'keywords_all': maamar.get('keywords_all', []),
                'embedding': maamar.get('embedding'),
                'score': score,
                'fuzzy_score': score,
                'keyword_score': 0,
                'semantic_score': 0,
                'words_found': words_found_count,
                'total_words': len(query_words)
            })
    
    # ××™×™×Ÿ ×œ×¤×™ ×¦×™×•×Ÿ (××”×’×‘×•×” ×œ× ××•×š) ×•×”×—×–×¨ ×¨×§ top_n
    results.sort(key=lambda x: x['score'], reverse=True)
    
    return results[:top_n]


def mara_makom_word_match_search(maamar_name: str, maamarim: Dict, top_n: int = 10) -> List[Dict]:
    """
    Track 1: exact-word matching for "××¨××” ××§×•×".
    Score is based on how many normalized query words appear in the maamar name (exact word match).
    """
    if not maamar_name:
        return []

    # ğŸ†• ×”×¡×¨ "×“'×”" ××• "×“"×”" ×•×›×•' ××”××¨××” ××§×•× (×¢× ××• ×‘×œ×™ ×¤×™×™×¤×™×)
    maamar_name = re.sub(r'\|?×“[×´"×³\'`â€²â€³â€´]×”\|?\s*', '', maamar_name).strip()
    
    # Expand abbreviations then normalize aggressively for Hebrew variations
    maamar_name_expanded = expand_abbreviations(maamar_name)
    query_norm = normalize_text(maamar_name_expanded, level=3)
    query_words = [w for w in query_norm.split() if w]
    if not query_words:
        return []

    print(f"ğŸ” mara_makom_word_match_search: '{maamar_name}' -> '{query_norm}' -> words: {query_words}")
    print(f"   ×‘×•×“×§ {len(maamarim)} ××××¨×™×...")

    results: List[Dict] = []
    checked_count = 0
    for key, maamar in maamarim.items():
        name = maamar.get('name', '') or ''
        if not name:
            continue

        checked_count += 1
        name_expanded = expand_abbreviations(name)
        name_norm = normalize_text(name_expanded, level=3)
        name_words = set(name_norm.split())

        words_found = sum(1 for w in query_words if w in name_words)
        if words_found <= 0:
            # ×œ×•×’ ×¨×§ ×œ××××¨×™× ×¨×œ×•×•× ×˜×™×™× (××›×™×œ×™× ×—×œ×§ ××”××™×œ×™×)
            if any(w in name_norm for w in query_words):
                print(f"   âš ï¸  '{name[:50]}' -> '{name_norm}' - ×œ× ×›×œ ×”××™×œ×™× × ××¦××• (×—×™×¤×©: {query_words})")
            continue

        total_words = len(query_words)
        score = int((words_found / total_words) * 100) if total_words else 0

        year_value = maamar.get('year')
        if not year_value:
            # Fallback: × ×¡×” ×œ×—×œ×¥ ××”×©× ×× ××™×Ÿ ×©×“×” year
            year_match = re.search(r'×ª×©[×-×ª]{1,2}(?:[×´"×³\'][×-×ª]|[×-×ª])?', name)
            if year_match:
                year_value = year_match.group(0)
        
        results.append({
            'key': key,
            'name': name,
            'filename': maamar.get('filename', ''),
            'text': maamar.get('text', ''),
            'year': year_value,
            'keywords_all': maamar.get('keywords_all', []),
            'embedding': maamar.get('embedding'),
            'score': score,
            'fuzzy_score': score,  # reuse field for UI display
            'keyword_score': 0,
            'semantic_score': 0,
            'words_found': words_found,
            'total_words': total_words,
        })

    print(f"   âœ… ×‘×“×§×ª×™ {checked_count} ××××¨×™×, ××¦××ª×™ {len(results)} ×ª×•×¦××•×ª")
    
    if not results:
        print(f"   âš ï¸  ××™×Ÿ ×ª×•×¦××•×ª ×œ×”×—×–×™×¨")
        return []
    
    # Sort by words_found desc, then by score desc, then by shorter names
    try:
        results.sort(key=lambda r: (r.get('words_found', 0), r.get('score', 0), -len(r.get('name', ''))), reverse=True)
        print(f"   ğŸ“Š ××—×¨×™ ××™×•×Ÿ: {len(results)} ×ª×•×¦××•×ª")
        if results:
            print(f"   ğŸ† ×”×ª×•×¦××” ×”×¨××©×•× ×”: '{results[0].get('name', '')[:50]}' (words_found={results[0].get('words_found', 0)}, score={results[0].get('score', 0)})")
    except Exception as e:
        print(f"   âŒ ×©×’×™××” ×‘××™×•×Ÿ: {e}")
        import traceback
        traceback.print_exc()
        return []
    
    final_results = results[:top_n]
    print(f"   ğŸ“¤ ××—×–×™×¨ {len(final_results)} ×ª×•×¦××•×ª (top_n={top_n})")
    return final_results


def keyword_search(
    question: str,
    candidates: List[Dict],
    require_ai_keywords: bool = False,
    forced_keywords: Optional[List[str]] = None
) -> tuple[List[Dict], Optional[List[str]]]:
    """
    ×—×™×¤×•×© ××©× ×™ ×‘××™×œ×•×ª ××¤×ª×— (×× ×™×© ×©××œ×”)
    
    ×œ×•×’×™×§×” ×—×“×©×”:
    - 100% = ×›×œ ×”××œ×™× ×©×—×™×¤×©× ×• ××•×¤×™×¢×•×ª ×‘×¨×¦×£ ×‘×ª×—×™×œ×ª ×¨×©×™××ª ×”××™×œ×•×ª ××¤×ª×—
    - 90% = ×›×œ ×”××œ×™× ××•×¤×™×¢×•×ª, ××‘×œ ×œ× ×‘×¨×¦×£ ××•×©×œ×
    - 80% = ×›×œ ×”××œ×™× ××•×¤×™×¢×•×ª, ××‘×œ ×‘××§×•××•×ª ×©×•× ×™×
    - X% = ×¨×§ ×—×œ×§ ××”××œ×™× ××•×¤×™×¢×•×ª (×™×—×¡×™ ×œ××¡×¤×¨ ×”××œ×™× ×©××¦×× ×•)
    
    Args:
        question: ×”×©××œ×” ×©×œ ×”××©×ª××©
        candidates: ×¨×©×™××ª ××××¨×™× ××•×¢××“×™× ××”×—×™×¤×•×© ×”×¨××©×•×Ÿ
    
    Returns:
        List[Dict]: ××××¨×™× ××“×•×¨×’×™× ××—×“×© ×œ×¤×™ ×”×ª×××” ×œ××™×œ×•×ª ××¤×ª×—
    """
    if not question or not candidates:
        return candidates, None
    
    # ğŸ¤– × ×¡×” ×œ×—×œ×¥ ××™×œ×•×ª ××¤×ª×— ×¢× Kimi/Groq (××• ×”×©×ª××© ×‘×¨×©×™××” ×©× ×›×¤×ª×” ××”×©×¨×ª-cache)
    ai_keywords = forced_keywords if (forced_keywords and isinstance(forced_keywords, list)) else extract_keywords_from_question(question)
    extracted_keywords = None  # ×©××•×¨ ××ª ××™×œ×•×ª ×”××¤×ª×— ×©×—×•×œ×¦×•
    
    # When we have AI-extracted keywords, we treat them as the "ground truth list":
    # score = (#keywords found exactly from this list) / (total keywords in this list) * 100
    ai_keywords_dedup: Optional[List[str]] = None
    if ai_keywords is not None and ai_keywords:
        seen_norm = set()
        dedup = []
        for kw in ai_keywords:
            kw = (kw or "").strip()
            if not kw:
                continue
            norm = normalize_text(kw, level=3).strip()
            if not norm:
                continue
            if norm in seen_norm:
                continue
            seen_norm.add(norm)
            dedup.append(kw)
        ai_keywords_dedup = dedup if dedup else None
        extracted_keywords = ai_keywords  # keep original list for UI display
        print(f"ğŸ” ××—×¤×© ×œ×¤×™ ××™×œ×•×ª ××¤×ª×— ×-OpenAI: {ai_keywords}")
    else:
        if require_ai_keywords:
            raise RuntimeError("Kimi/Groq keyword extraction failed (require_ai_keywords=true)")
        # Fallback - ×¡× ×Ÿ ××™×œ×•×ª ×¢×–×¨ ×™×“× ×™×ª
        normalized_question = normalize_text(question, level=3)
        all_words = normalized_question.split()
        
        # ××™×œ×•×ª ×¢×–×¨ ×œ×¡×™× ×•×Ÿ
        stop_words = {
            '××”', '××™', '××™×š', '×œ××”', '×”××', '××™×¤×”', '××ª×™', '×›××”',
            '×©×œ', '×¢×œ', '××ª', '×¢×', '×œ×¤×™', '××œ', '××Ÿ', '×‘', '×œ', '×›', '×',
            '×”×¨×‘', '×¨×‘', '×“×¢×ª', '××•××¨', '××¡×‘×™×¨', '××“×‘×¨', '××•××¨×™×',
            '×–×”', '×–×•', '×–××ª', '××œ×”', '××œ×•',
            '×›×œ', '×›×•×œ×', '×›×•×œ×Ÿ', '×”×›×œ',
            '×™×©', '××™×Ÿ', '×™×”×™×”', '×”×™×”',
            '××•', '×•×’×', '××‘×œ', '×¨×§', '×’×', '××£', '×›×™', '××', '×©'
        }
        
        # ×¡× ×Ÿ ××™×œ×•×ª ×¢×–×¨
        question_words_list = [w for w in all_words if w not in stop_words and len(w) > 1]
        question_words_set = set(question_words_list)
        # ğŸ†• ×”×—×–×¨ ×’× ×‘-fallback ××ª ××™×œ×•×ª ×”××¤×ª×— ×‘×¤×•×¢×œ ×©×”×©×ª××©× ×• ×‘×”×Ÿ (×›×“×™ ×©×™×•×¤×™×¢×• ×‘-UI)
        # (×“×™×œ×•×’ ×¢×œ ×›×¤×™×œ×•×™×•×ª ×•×©××™×¨×ª ×¡×“×¨)
        seen = set()
        fallback_keywords: List[str] = []
        for w in question_words_list:
            if w in seen:
                continue
            seen.add(w)
            fallback_keywords.append(w)
        extracted_keywords = fallback_keywords
        print(f"ğŸ” ××—×¤×© ×œ×¤×™ ××™×œ×•×ª ×”×©××œ×” (××—×¨×™ ×¡×™× ×•×Ÿ): {question_words_list}")
    
    # ×“×¨×’ ××—×“×© ×œ×¤×™ ××™×œ×•×ª ××¤×ª×—
    for candidate in candidates:
        keywords = candidate.get('keywords_all', [])
        matching_words = set()
        matching_original = []
        keyword_score = 0
        # Track 2 ranking helpers (question keyword list mode)
        min_count = 0
        sum_count = 0
        
        # ğŸ†• Mapping: question keyword -> table keyword (for text search)
        # This maps "×™×•× ×ª×Ÿ" -> "×™×”×•× ×ª×Ÿ" if found in keywords_all via normalized matching
        question_to_table_keyword = {}  # Maps question keyword -> table keyword
        
        # ×©×œ×‘ 1: ×—×¤×© ×‘××™×œ×•×ª ××¤×ª×— (×× ×™×©)
        if keywords:
            # ğŸ†• ×”×©×ª××© ×‘-keywords_all_normalized ×× ×§×™×™× (××”×˜×‘×œ×”), ××—×¨×ª × ×¨××œ ×¢×›×©×™×•
            keywords_normalized = candidate.get('keywords_all_normalized', [])
            if keywords_normalized and len(keywords_normalized) == len(keywords):
                # ×™×© ×¢××•×“×ª × ×¨××•×œ ××”×˜×‘×œ×” - ×”×©×ª××© ×‘×”
                normalized_keywords = keywords_normalized
            else:
                # ××™×Ÿ ×¢××•×“×ª × ×¨××•×œ - × ×¨××œ ×¢×›×©×™×•
                normalized_keywords = [normalize_text(kw, level=3) for kw in keywords]
            all_keyword_words = ' '.join(normalized_keywords).split()
            keywords_set = set(all_keyword_words)
            
            # ×¦×•×¨ ××™×¤×•×™ ×× ×•×¨××œ -> ××§×•×¨×™
            normalized_to_original = {}
            for i, kw in enumerate(keywords):
                # ğŸ†• ×”×©×ª××© ×‘-keywords_all_normalized ×× ×§×™×™×
                if keywords_normalized and i < len(keywords_normalized):
                    normalized = keywords_normalized[i]
                else:
                    normalized = normalize_text(kw, level=3)
                # ××™×¤×•×™ ×’× ×œ×¤×™ ×”××—×¨×•×–×ª ×•×’× ×œ×¤×™ ×›×œ ××™×œ×” ×‘×ª×•×š ×”××—×¨×•×–×ª
                if normalized and normalized not in normalized_to_original:
                    normalized_to_original[normalized] = kw
                for t in normalized.split():
                    if t and t not in normalized_to_original:
                        normalized_to_original[t] = kw
            
            # Match phase:
            # - If we have AI keyword list: determine "found" by counting exact mentions in the maamar text.
            # - Otherwise: fall back to token matching against keywords_all.
            if ai_keywords_dedup:
                # Track 2: fuzzy match extracted keywords against the maamar keyword list
                # (then counts are taken from maamar text for display/tie-break)
                FUZZY_THRESHOLD = 85
                # ğŸ†• ×”×©×ª××© ×‘-keywords_all_normalized ×× ×§×™×™× (××”×˜×‘×œ×”), ××—×¨×ª × ×¨××œ ×¢×›×©×™×•
                keywords_normalized = candidate.get('keywords_all_normalized', [])
                if keywords_normalized and len(keywords_normalized) == len(keywords):
                    # ×™×© ×¢××•×“×ª × ×¨××•×œ ××”×˜×‘×œ×” - ×”×©×ª××© ×‘×”
                    cand_norm_kws = keywords_normalized
                else:
                    # ××™×Ÿ ×¢××•×“×ª × ×¨××•×œ - × ×¨××œ ×¢×›×©×™×•
                    cand_norm_kws = [normalize_text(kw, level=3) for kw in keywords if kw]
                found_phrases = []
                for kw in ai_keywords_dedup:
                    qn = normalize_text(kw, level=3).strip()
                    if not qn:
                        continue
                    
                    # First: try exact match in keywords_all
                    exact_match = None
                    if kw in keywords:
                        exact_match = kw
                    else:
                        # Try normalized match: normalize both question keyword and table keywords
                        # This allows "×™×•× ×ª×Ÿ" to match "×™×”×•× ×ª×Ÿ" in the table
                        for i, table_kw in enumerate(keywords):
                            if not table_kw:
                                continue
                            table_norm = cand_norm_kws[i]
                            # Check if normalized versions match (e.g., "×™×•× ×ª×Ÿ" == "×™×”×•× ×ª×Ÿ" after normalization)
                            if qn == table_norm:
                                exact_match = table_kw
                                break
                    
                    # If exact match found (exact or normalized), use it
                    if exact_match:
                        found_phrases.append(kw)
                        question_to_table_keyword[kw] = exact_match
                        # Debug: print when we map a question keyword to a table keyword
                        if exact_match != kw:
                            print(f"ğŸ” ××™×¤×•×™ ××™×œ×ª ××¤×ª×—: '{kw}' -> '{exact_match}' (×”×ª×××” ×× ×•×¨××œ×™×ª)")
                    else:
                        # Fallback: fuzzy match (only if normalized match didn't work)
                        best = 0
                        best_table_kw = None
                        for i, cn in enumerate(cand_norm_kws):
                            if not cn:
                                continue
                            # ratio works well for keywords; partial_ratio too permissive in Hebrew
                            try:
                                s = fuzz.ratio(qn, cn)
                            except Exception:
                                s = 0
                            if s > best:
                                best = s
                                best_table_kw = keywords[i] if i < len(keywords) else None
                                if best >= 100:
                                    break
                        if best >= FUZZY_THRESHOLD and best_table_kw:
                            found_phrases.append(kw)
                            question_to_table_keyword[kw] = best_table_kw
                            print(f"ğŸ” ××™×¤×•×™ ××™×œ×ª ××¤×ª×— (fuzzy): '{kw}' -> '{best_table_kw}' (×¦×™×•×Ÿ: {best}%)")

                matching_words = set(found_phrases)
            else:
                matching_words = question_words_set.intersection(keywords_set)
            
            # ×”××¨ ×—×–×¨×” ×œ××™×œ×™× ××§×•×¨×™×•×ª ××”××××¨
            matching_original = [normalized_to_original.get(w, w) for w in matching_words]
            
            if matching_words:
                # ×—×©×‘ ×¦×™×•×Ÿ ×œ×¤×™ ×›××” ××œ×™× ××¦×× ×•
                if ai_keywords_dedup:
                    total = len(ai_keywords_dedup) or 1
                    found_ratio = len(matching_words) / total
                else:
                    found_ratio = len(matching_words) / len(question_words_list)

                # âœ… score by how many keywords from the question-list were found
                keyword_score = int(found_ratio * 100)
        
        # ğŸ†• ×× ×™×© AI keywords, ×ª××™×“ ×—×¤×© ×‘×˜×§×¡×˜ ×”××œ× ×›×“×™ ×œ×¡×¤×•×¨ ××ª ×›×œ ×”××•×¤×¢×™×
        # ×–×” ××‘×˜×™×— ×©××××¨×™× ×©×™×© ×‘×”× ××ª ××™×œ×ª ×”××¤×ª×— ×‘×˜×§×¡×˜ ×™×™×›×œ×œ×•, ×’× ×× ×”×™× ×œ× ×‘-keywords_all
        counts_all = {}
        if ai_keywords_dedup:
            text = candidate.get('text', '')
            if text:
                # ğŸ†• ×‘× ×” ×¨×©×™××ª ××™×œ×•×ª ××¤×ª×— ×œ×—×™×¤×•×© ×‘×˜×§×¡×˜:
                # ×× ×™×© ×”×ª×××” ×‘×˜×‘×œ×” (keywords_all), ×”×©×ª××© ×‘××™×œ×” ××”×˜×‘×œ×”
                # ××—×¨×ª, ×”×©×ª××© ×‘××™×œ×” ××”×©××œ×”
                phrases_to_search = []
                for kw in ai_keywords_dedup:
                    # ×× ×™×© ××™×¤×•×™ ×œ××™×œ×” ××”×˜×‘×œ×”, ×”×©×ª××© ×‘×”
                    if kw in question_to_table_keyword:
                        table_kw = question_to_table_keyword[kw]
                        phrases_to_search.append(table_kw)
                        if table_kw != kw:
                            print(f"ğŸ” ×—×™×¤×•×© ×‘×˜×§×¡×˜: '{kw}' -> '{table_kw}' (××”×˜×‘×œ×”)")
                    else:
                        phrases_to_search.append(kw)
                
                # ×—×¤×© ×‘×˜×§×¡×˜ ××ª ×”××™×œ×™× (××”×˜×‘×œ×” ××• ××”×©××œ×”)
                counts_all_raw = _count_phrase_mentions_in_text(text, phrases_to_search)
                
                # ğŸ†• ××™×¤×•×™ ×—×–×¨×”: ××™×œ×ª ××¤×ª×— ××”×©××œ×” -> ××¡×¤×¨ ××•×¤×¢×™×
                # (×× ×—×™×¤×©× ×• "×™×”×•× ×ª×Ÿ" ××”×˜×‘×œ×”, × ×©××•×¨ ××ª ×”××¡×¤×¨ ×ª×—×ª "×™×•× ×ª×Ÿ" ××”×©××œ×”)
                for i, kw in enumerate(ai_keywords_dedup):
                    phrase_searched = phrases_to_search[i]
                    count = counts_all_raw.get(phrase_searched, 0)
                    counts_all[kw] = count
                
                # ğŸ†• ×¢×“×›×Ÿ ××ª matching_words ×’× ×œ×¤×™ ××” ×©× ××¦× ×‘×˜×§×¡×˜ (×œ× ×¨×§ keywords_all)
                # ×–×” ××‘×˜×™×— ×©×›×œ ××™×œ×ª ××¤×ª×— ×©× ××¦××” ×‘×˜×§×¡×˜ ×ª×™×›×œ×œ, ×’× ×× ×œ× ×‘-keywords_all
                found_in_text = [kw for kw in ai_keywords_dedup if int(counts_all.get(kw, 0) or 0) > 0]
                if found_in_text:
                    # ×”×•×¡×£ ××ª ×›×œ ×”××™×œ×™× ×©× ××¦××• ×‘×˜×§×¡×˜ ×œ-matching_words
                    matching_words = matching_words.union(set(found_in_text))
                
                # ×—×©×‘ ×¦×™×•×Ÿ ×œ×¤×™ ×›××” ××™×œ×•×ª ××¤×ª×— × ××¦××• (×‘-keywords_all ××• ×‘×˜×§×¡×˜)
                # ×–×” ××ª×‘×¦×¢ ×ª××™×“, ×’× ×× matching_words ×¨×™×§ (××– ×”×¦×™×•×Ÿ ×™×”×™×” 0)
                total = len(ai_keywords_dedup) or 1
                found_ratio = len(matching_words) / total if matching_words else 0
                keyword_score = int(found_ratio * 100)
        
        # ×¢×“×›×Ÿ ××ª ×”-candidate ×× × ××¦××• ×”×ª×××•×ª
        if matching_words:
            candidate['keyword_score'] = keyword_score
            
            # ×©××•×¨ ××ª ×”××™×œ×™× ×©× ××¦××•
            if ai_keywords_dedup:
                # In AI-keyword mode, "matched_keywords" are from the question keyword list (only those found).
                matched_list = [kw for kw in ai_keywords_dedup if kw in matching_words]
                # Store counts for ALL keywords (for consistent table display)
                candidate['matched_keywords'] = matched_list
                candidate['matched_keyword_counts'] = {kw: int(counts_all.get(kw, 0) or 0) for kw in ai_keywords_dedup}

                # Tie-break helpers: prefer balanced coverage (bottleneck), then overall mentions
                all_counts = [int(counts_all.get(kw, 0) or 0) for kw in ai_keywords_dedup]
                min_count = min(all_counts) if all_counts else 0
                sum_count = sum(all_counts) if all_counts else 0
                candidate['_min_kw_count'] = min_count
                candidate['_sum_kw_count'] = sum_count
            else:
                matched_list = sorted(list(set(matching_original)))
                candidate['matched_keywords'] = matched_list
                # ğŸ†• ×”×¦××“ ×œ×›×œ ××™×œ×ª ××¤×ª×— ×’× ××ª ××¡×¤×¨ ×”××™×–×›×•×¨×™× ×©×œ×” ×‘×ª×•×š ×˜×§×¡×˜ ×”××××¨
                candidate['matched_keyword_counts'] = _count_phrase_mentions_in_text(
                    candidate.get('text', ''),
                    matched_list
                )
            
            # ğŸ†• ××¦×‘ "×©××œ×” ×›×œ×œ×™×ª" (×œ×œ× fuzzy): ×§×‘×¢ ××ª ×”×¦×™×•×Ÿ ×œ×¤×™ ××—×•×– ××™×œ×•×ª ×”××¤×ª×— ×©× ××¦××•
            # ×›×“×™ ×©×”×¦×™×•×Ÿ ×©×™×•×¤×™×¢/×™×™×‘×—×¨ ×™×ª××™× ×œ-ğŸ”‘XX% ×©××•×¦×’ ×‘×˜×‘×œ×”.
            if (candidate.get('fuzzy_score', 0) or 0) == 0 and (candidate.get('score', 0) or 0) == 50:
                candidate['score'] = keyword_score
            else:
                # ××¦×‘ ×¨×’×™×œ: ×”×•×¡×£ ×‘×•× ×•×¡ ×œ×¦×™×•×Ÿ ×”×›×•×œ×œ (×¢×“ 20%)
                bonus = int((keyword_score / 100) * 20)
                candidate['score'] = min(100, candidate['score'] + bonus)
    
    # ××™×™×Ÿ ××—×“×©
    if ai_keywords_dedup:
        # Track 2: sort by score desc, then min-count desc, then sum-count desc
        candidates.sort(
            key=lambda x: (
                x.get('score', 0),
                x.get('_min_kw_count', 0),
                x.get('_sum_kw_count', 0),
            ),
            reverse=True
        )
        # cleanup internal fields
        for c in candidates:
            c.pop('_min_kw_count', None)
            c.pop('_sum_kw_count', None)
    else:
        candidates.sort(key=lambda x: x['score'], reverse=True)
    
    return candidates, extracted_keywords


def _tokenize_query_keywords(question: str, extracted_keywords: Optional[List[str]]) -> List[str]:
    """
    Build a token list to use for tie-breaking based on keyword mentions in the maamar text.
    - If extracted_keywords exist (AI keywords), normalize+split them into word tokens.
    - Otherwise, fallback to normalized question words with stop-words removed.
    Returns a de-duplicated list preserving order.
    """
    tokens: List[str] = []
    seen = set()

    if extracted_keywords:
        for kw in extracted_keywords:
            norm = normalize_text(kw, level=3)
            for t in norm.split():
                t = t.strip()
                if not t:
                    continue
                if t in seen:
                    continue
                seen.add(t)
                tokens.append(t)
        return tokens

    # Fallback to question words (similar to keyword_search fallback)
    normalized_question = normalize_text(question or "", level=3)
    all_words = normalized_question.split()

    stop_words = {
        '××”', '××™', '××™×š', '×œ××”', '×”××', '××™×¤×”', '××ª×™', '×›××”',
        '×©×œ', '×¢×œ', '××ª', '×¢×', '×œ×¤×™', '××œ', '××Ÿ', '×‘', '×œ', '×›', '×',
        '×”×¨×‘', '×¨×‘', '×“×¢×ª', '××•××¨', '××¡×‘×™×¨', '××“×‘×¨', '××•××¨×™×',
        '×–×”', '×–×•', '×–××ª', '××œ×”', '××œ×•',
        '×›×œ', '×›×•×œ×', '×›×•×œ×Ÿ', '×”×›×œ',
        '×™×©', '××™×Ÿ', '×™×”×™×”', '×”×™×”',
        '××•', '×•×’×', '××‘×œ', '×¨×§', '×’×', '××£', '×›×™', '××', '×©'
    }

    base = [w for w in all_words if w not in stop_words and len(w) > 1]

    # Also consider the word without a leading ×•' ×”×—×™×‘×•×¨
    expanded: List[str] = []
    for w in base:
        expanded.append(w)
        if w.startswith('×•') and len(w) > 2:
            expanded.append(w[1:])

    for t in expanded:
        if not t:
            continue
        if t in seen:
            continue
        seen.add(t)
        tokens.append(t)

    return tokens


def _count_keyword_mentions_in_text(text: str, tokens: List[str]) -> int:
    """
    Count how many times keyword tokens appear in the maamar text (word-level).
    Uses normalize_text(level=3) so punctuation/quotes are removed and tokens are space-separated.
    """
    if not text or not tokens:
        return 0
    import re
    norm_text = normalize_text(text, level=3)
    if not norm_text:
        return 0
    total = 0
    for tok in tokens:
        if not tok:
            continue
        # Match whole token as a word (space-delimited after normalization)
        pattern = re.compile(rf"(?<!\S){re.escape(tok)}(?!\S)")
        total += len(pattern.findall(norm_text))
    return total


def _remove_numbers_from_keyword(keyword: str) -> str:
    """
    ×”×¡×¨ ××¡×¤×¨×™× ×××™×œ×ª ××¤×ª×—.
    ×œ×“×•×’××”: "××—×“ ×¢×©×¨×” ×‘×—×™× ×•×ª" -> "×‘×—×™× ×•×ª"
    """
    if not keyword:
        return keyword
    
    # ×¨×©×™××ª ×¦×™×¨×•×¤×™ ××¡×¤×¨×™× (×¦×¨×™×š ×œ×‘×“×•×§ ×§×•×“× ××ª ×”×¦×™×¨×•×¤×™× ×”××¨×•×›×™×)
    number_phrases = [
        '××—×“ ×¢×©×¨×”', '××—×“ ×¢×©×¨', '×©×ª×™×™× ×¢×©×¨×”', '×©×ª×™×™× ×¢×©×¨', '×©×œ×•×© ×¢×©×¨×”', '×©×œ×•×© ×¢×©×¨',
        '××¨×‘×¢ ×¢×©×¨×”', '××¨×‘×¢ ×¢×©×¨', '×—××© ×¢×©×¨×”', '×—××© ×¢×©×¨', '×©×© ×¢×©×¨×”', '×©×© ×¢×©×¨',
        '×©×‘×¢ ×¢×©×¨×”', '×©×‘×¢ ×¢×©×¨', '×©××•× ×” ×¢×©×¨×”', '×©××•× ×” ×¢×©×¨', '×ª×©×¢ ×¢×©×¨×”', '×ª×©×¢ ×¢×©×¨',
        '×©×œ×•×© ×××•×ª', '××¨×‘×¢ ×××•×ª', '×—××© ×××•×ª', '×©×© ×××•×ª', '×©×‘×¢ ×××•×ª', '×©××•× ×” ×××•×ª', '×ª×©×¢ ×××•×ª'
    ]
    
    # ×¨×©×™××ª ××™×œ×™× ×©××ª××¨×•×ª ××¡×¤×¨×™× ×‘×¢×‘×¨×™×ª (××™×œ×™× ×‘×•×“×“×•×ª)
    number_words = {
        '××—×“', '×©×ª×™×™×', '×©×œ×•×©', '××¨×‘×¢', '×—××©', '×©×©', '×©×‘×¢', '×©××•× ×”', '×ª×©×¢', '×¢×©×¨',
        '×¢×©×¨×”', '×¢×©×¨×™×', '×©×œ×•×©×™×', '××¨×‘×¢×™×', '×—××™×©×™×', '×©×™×©×™×', '×©×‘×¢×™×', '×©××•× ×™×', '×ª×©×¢×™×',
        '×××”', '×××ª×™×™×',
        '×™×', '×™×‘', '×™×’', '×™×“', '×˜×•', '×˜×–', '×™×–', '×™×—', '×™×˜', '×›', '×œ'
    }
    
    result = keyword
    
    # ×§×•×“× ×›×œ, ×”×¡×¨ ×¦×™×¨×•×¤×™ ××¡×¤×¨×™× (×œ××©×œ "××—×“ ×¢×©×¨×”")
    # ×¦×¨×™×š ×œ×‘×“×•×§ ××ª ×”×¦×™×¨×•×¤×™× ×”××¨×•×›×™× ×§×•×“×
    for phrase in sorted(number_phrases, key=len, reverse=True):
        if phrase in result:
            # ×”×¡×¨ ××ª ×”×¦×™×¨×•×£ ×›×•×œ×• (×¢× ×¨×•×•×—×™× ×œ×¤× ×™ ×•××—×¨×™)
            result = result.replace(f' {phrase} ', ' ').replace(f'{phrase} ', '').replace(f' {phrase}', '')
    
    # ××—×¨ ×›×š, ×”×¡×¨ ××™×œ×™× ×‘×•×“×“×•×ª ×©××ª××¨×•×ª ××¡×¤×¨×™×
    words = result.split()
    filtered_words = [w for w in words if w not in number_words]
    
    # ×”×—×–×¨ ××ª ×”××™×œ×™× ×”× ×•×ª×¨×•×ª (×¢× ×¨×•×•×—×™×)
    result_final = ' '.join(filtered_words).strip()
    
    # ×× ×”×ª×•×¦××” ×¨×™×§×” ××• ×–×”×” ×œ××§×•×¨, ×”×—×–×¨ ××ª ×”××§×•×¨
    if not result_final or result_final == keyword:
        return keyword
    
    return result_final


def _count_phrase_mentions_in_text(text: str, phrases: List[str]) -> Dict[str, int]:
    """
    Count mentions for each phrase (original keyword string) inside a maamar text.
    
    Search strategy:
    1. Search for the exact keyword phrase (as it appears in the global keyword table)
    2. If the keyword has an abbreviation, also search for the exact abbreviation
    3. If the keyword contains numbers, also search for the keyword without numbers
    
    No normalization is performed - exact matching only.
    """
    if not text or not phrases:
        return {}
    import re
    
    counts: Dict[str, int] = {}
    # Use the PKL-built alias table so abbreviations and full terms are counted together reliably.
    norm_text = normalize_text(text, level=3)
    
    for ph in phrases:
        if not ph:
            continue

        ph0 = ph.strip()
        aliases = []
        try:
            aliases = _KEYWORD_ALIASES.get(ph0) or []
        except Exception:
            aliases = []
        if not aliases:
            aliases = [ph0]

        total = 0
        for a in aliases:
            a_norm = normalize_text(a, level=3)
            if not a_norm:
                continue
            total += len(list(re.compile(re.escape(a_norm)).finditer(norm_text)))

        counts[ph0] = int(total)
    
    return counts


def _rank_ties_by_keyword_mentions(results: List[Dict], *, tokens: List[str]) -> List[Dict]:
    """
    For equal 'score' groups, rank by keyword-mention count in 'text' (desc).
    Keeps the overall 'score' ordering intact.
    """
    if not results or not tokens:
        return results

    # Precompute counts once per item
    counts = [_count_keyword_mentions_in_text(r.get("text", ""), tokens) for r in results]

    # Stable sort by (score desc, mentions desc)
    indexed = list(enumerate(results))
    indexed.sort(key=lambda t: (t[1].get("score", 0), counts[t[0]]), reverse=True)
    return [t[1] for t in indexed]


def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    """
    ×—×©×‘ ×“××™×•×Ÿ ×§×•×¡×™× ×•×¡×™ ×‘×™×Ÿ 2 ×•×§×˜×•×¨×™×
    
    Args:
        vec1, vec2: ×•×§×˜×•×¨×™× (×¨×©×™××•×ª ×©×œ ××¡×¤×¨×™×)
    
    Returns:
        float: ×¦×™×•×Ÿ ×“××™×•×Ÿ (0-1)
    """
    if not HAS_NUMPY:
        return 0.0
    
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)
    
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    
    if norm1 == 0 or norm2 == 0:
        return 0.0
    
    return float(dot_product / (norm1 * norm2))


def semantic_search(question: str, candidates: List[Dict], api_key: Optional[str] = None) -> List[Dict]:
    """
    ×—×™×¤×•×© ×¡×× ×˜×™ ×‘×××¦×¢×•×ª OpenAI embeddings
    
    Args:
        question: ×”×©××œ×” ×©×œ ×”××©×ª××©
        candidates: ×¨×©×™××ª ××××¨×™× ××•×¢××“×™×
        api_key: OpenAI API key (None = ×-environment variable)
    
    Returns:
        List[Dict]: ××××¨×™× ××“×•×¨×’×™× ×œ×¤×™ ×“××™×•×Ÿ ×¡×× ×˜×™
    """
    if not question or not candidates:
        return candidates

    if not ENABLE_SEMANTIC_SEARCH:
        # ×‘×¨×™×¨×ª ××—×“×œ: ×›×‘×•×™ ×‘×§×•×‘×¥ "2_" (×›×“×™ ×œ× ×œ×¢×¨×‘×‘ embeddings ××¡×¤×§ ××—×¨)
        return candidates
    
    # ×‘×“×•×§ ×©×™×© embeddings ×œ××××¨×™×
    candidates_with_embeddings = [c for c in candidates if c.get('embedding')]
    if not candidates_with_embeddings:
        print("âš ï¸  ××™×Ÿ embeddings ×œ××××¨×™× - ××“×œ×’ ×¢×œ ×—×™×¤×•×© ×¡×× ×˜×™")
        return candidates
    
    # ×‘×“×•×§ ×©×™×© OpenAI
    if not HAS_OPENAI or not HAS_NUMPY:
        print("âš ï¸  OpenAI ××• numpy ×œ× ×–××™× ×™× - ××“×œ×’ ×¢×œ ×—×™×¤×•×© ×¡×× ×˜×™")
        return candidates
    
    try:
        # ×¦×•×¨ embedding ×œ×©××œ×”
        client = OpenAI(api_key=api_key) if api_key else OpenAI()
        
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=question
        )
        
        question_embedding = response.data[0].embedding
        
        # ×—×©×‘ ×“××™×•×Ÿ ×œ×›×œ ××××¨
        for candidate in candidates_with_embeddings:
            maamar_embedding = candidate['embedding']
            similarity = cosine_similarity(question_embedding, maamar_embedding)
            
            # ×”××¨ ×œ-% ×•×”×•×¡×£ ×œ×¦×™×•×Ÿ
            semantic_score = int(similarity * 100)
            
            # ×©×œ×‘ ×¢× ×”×¦×™×•×Ÿ ×”×§×™×™× (70% fuzzy + 30% semantic)
            candidate['score'] = int(candidate['score'] * 0.7 + semantic_score * 0.3)
            candidate['semantic_score'] = semantic_score
        
        # ××™×™×Ÿ ××—×“×©
        candidates_with_embeddings.sort(key=lambda x: x['score'], reverse=True)
        
        return candidates_with_embeddings
        
    except Exception as e:
        print(f"âš ï¸  ×©×’×™××” ×‘×—×™×¤×•×© ×¡×× ×˜×™: {e}")
        return candidates


def search_by_year(year: str, 
                   max_results: int = 100,
                   pkl_source: Optional[str] = None) -> List[Dict]:
    """
    ×—×™×¤×•×© ×›×œ ×”××××¨×™× ×××•×ª×” ×©× ×”
    
    Args:
        year: ×”×©× ×” ×œ×—×™×¤×•×© (×œ××©×œ: "×ª×©×œ×´×—" ××• "×ª×©×œ×—")
        max_results: ××¡×¤×¨ ×ª×•×¦××•×ª ××§×¡×™××œ×™ (×‘×¨×™×¨×ª ××—×“×œ: 100)
        pkl_source: × ×ª×™×‘ ××• URL ×œ-PKL (None = ××•×˜×•××˜×™)
    
    Returns:
        List[Dict]: ×¨×©×™××ª ×›×œ ×”××××¨×™× ×××•×ª×” ×©× ×”
    
    Examples:
        >>> results = search_by_year("×ª×©×œ×´×—")
        >>> # ××—×–×™×¨ ××ª ×›×œ ×”××××¨×™× ××©× ×ª ×ª×©×œ"×—
    """
    # ×˜×¢×Ÿ ××××¨×™×
    maamarim = load_maamarim(pkl_source)
    if not maamarim:
        print("âŒ ×œ× × ×™×ª×Ÿ ×œ×˜×¢×•×Ÿ ××××¨×™×")
        return []
    
    # × ×¨××œ ××ª ×”×©× ×” (×”×¡×¨ ×”', ×’×¨×©×™×™×, ×•×›×•')
    year_normalized = year
    year_normalized = re.sub(r"^×”['\×³]?", '', year_normalized)  # ×”×¡×¨ "×”" ×‘×”×ª×—×œ×”
    year_normalized = re.sub(r'[×´"\'×³/]', '', year_normalized)  # ×”×¡×¨ ×’×¨×©×™×™× ×•×¡×™×× ×™×
    
    print(f"ğŸ” ××—×¤×© ××ª ×›×œ ×”××××¨×™× ××©× ×ª: {year}")
    print(f"   ×©× ×” ×× ×•×¨××œ×ª: {year_normalized}")
    
    results = []
    for key, maamar in maamarim.items():
        name = maamar.get('name', '')
        if not name:
            continue
        
        # ğŸ†• ×§×‘×œ ××ª ×”×©× ×” ×™×©×™×¨×•×ª ××”-PKL (×× ×§×™×™××ª)
        maamar_year_from_pkl = maamar.get('year', None)
        
        if maamar_year_from_pkl:
            # ×™×© ×©×“×” year ×‘-PKL - ×”×©×ª××© ×‘×•!
            maamar_year_normalized = re.sub(r"^×”['\×³]?", '', maamar_year_from_pkl)
            maamar_year_normalized = re.sub(r'[×´"\'×³/]', '', maamar_year_normalized)
            
            # ×”×©×•×•××” ××“×•×™×§×ª
            if maamar_year_normalized == year_normalized:
                results.append({
                    'name': name,
                    'text': maamar.get('text', ''),
                    'filename': key,
                    'year': maamar_year_from_pkl,  # ×©× ×” ××”-PKL
                    'keywords_all': maamar.get('keywords_all', []),  # ğŸ†• ××™×œ×•×ª ××¤×ª×—
                    'score': 100,
                    'fuzzy_score': 0,
                    'keyword_score': 0,
                    'semantic_score': 0
                })
        else:
            # ××™×Ÿ ×©×“×” year ×‘-PKL - Fallback: ×—×œ×¥ ××”×©×
            year_match = re.search(r'×ª×©[×-×ª]{1,2}(?:[×´"×³\'][×-×ª]|[×-×ª])?', name)
            if year_match:
                maamar_year = year_match.group(0)
                maamar_year_normalized = re.sub(r"^×”['\×³]?", '', maamar_year)
                maamar_year_normalized = re.sub(r'[×´"\'×³/]', '', maamar_year_normalized)
                
                if maamar_year_normalized == year_normalized:
                    results.append({
                        'name': name,
                        'text': maamar.get('text', ''),
                        'filename': key,
                        'year': maamar_year,  # ×©× ×” ××”×©×
                        'keywords_all': maamar.get('keywords_all', []),  # ğŸ†• ××™×œ×•×ª ××¤×ª×—
                        'score': 100,
                        'fuzzy_score': 0,
                        'keyword_score': 0,
                        'semantic_score': 0
                    })
    
    # ××™×™×Ÿ ×œ×¤×™ ×©× (××œ×¤×‘×™×ª×™×ª)
    results.sort(key=lambda x: x['name'])
    
    print(f"âœ… × ××¦××• {len(results)} ××××¨×™× ××©× ×ª {year}")
    
    # ×”×’×‘×œ ×œ××¡×¤×¨ ×”××§×¡×™××œ×™
    if len(results) > max_results:
        results = results[:max_results]
    
    return results


def search_maamar(maamar_name: str, 
                  year: Optional[str] = None,
                  question: Optional[str] = None, 
                  max_results: int = 5, 
                  min_score: int = 0,
                  pkl_source: Optional[str] = None,
                  use_semantic: bool = True,
                  openai_api_key: Optional[str] = None) -> List[Dict]:
    """
    ×—×™×¤×•×© ××××¨×™× - ×¤×•× ×§×¦×™×” ×¨××©×™×ª
    
    ×œ×•×’×™×§×ª ×”×—×™×¤×•×©:
    ================
    1. ×× ×™×© ×©× ××××¨:
       - Fuzzy Search ×¢×œ ×¢××•×“×ª ×©××•×ª ×”××××¨×™×
       - ×× × ××¦××” ×”×ª×××” ××•×©×œ××ª (100%) ×œ××××¨ ××—×“ â†’ ××—×–×™×¨ ××™×“
       - ×× × ××¦××• ×™×•×ª×¨ ×××××¨ ××—×“ â†’ ××“×¨×’ ×œ×¤×™ Keywords + OpenAI
    
    2. ×× ××™×Ÿ ×©× ××××¨ (×©××œ×” ×›×œ×œ×™×ª):
       - ××—×–×™×¨ ××ª ×›×œ ×”××××¨×™×
       - ××“×¨×’ ×œ×¤×™ Keywords (Grok) + OpenAI Semantic
    
    Args:
        maamar_name: ×©× ×”××××¨ ×œ×—×™×¤×•×© (×’×•×œ××™ - ×¢× "××××¨", ×¢× ×©× ×”, ×•×›×•')
        year: ×©× ×” ×œ×—×™×¤×•×© (××•×¤×¦×™×•× ×œ×™ - ×× None, ×™× ×¡×” ×œ×—×œ×¥ ×-maamar_name)
        question: ×©××œ×” ×œ×—×™×¤×•×© ×‘×ª×•×›×Ÿ ×”××××¨×™× (××•×¤×¦×™×•× ×œ×™)
        max_results: ××¡×¤×¨ ×ª×•×¦××•×ª ××§×¡×™××œ×™
        min_score: ×¦×™×•×Ÿ ××™× ×™××œ×™ ×œ×”×—×–×¨×” (×‘×¨×™×¨×ª ××—×“×œ: 0 = ××™×Ÿ ×¡×£)
        pkl_source: × ×ª×™×‘ ××• URL ×œ-PKL (None = ××•×˜×•××˜×™)
        use_semantic: ×”×× ×œ×”×©×ª××© ×‘×—×™×¤×•×© ×¡×× ×˜×™ OpenAI (×‘×¨×™×¨×ª ××—×“×œ: ×›×Ÿ)
        openai_api_key: OpenAI API key (None = ×-environment variable)
    
    Returns:
        List[Dict]: ×¨×©×™××ª ××××¨×™× ××ª××™××™× (×××•×™× ×™× ×œ×¤×™ ×¦×™×•×Ÿ ×™×•×¨×“)
    
    Examples:
        >>> # ×—×™×¤×•×© ××××¨ ×¡×¤×¦×™×¤×™ (×’×•×œ××™)
        >>> results = search_maamar("××××¨ ×•××‘×¨×”× ×–×§×Ÿ ×ª×©×œ×´×•")
        
        >>> # ××××¨ ×¢× ×©× ×” × ×¤×¨×“×ª
        >>> results = search_maamar("×•××‘×¨×”× ×–×§×Ÿ", year="×ª×©×œ×´×•")
        
        >>> # ××××¨ ×¢× ×©××œ×”
        >>> results = search_maamar("×‘××ª×™ ×œ×’× ×™", question="×§×™×•× ×”××¦×•×•×ª")
        
        >>> # ×©××œ×” ×›×œ×œ×™×ª (×œ×œ× ×©× ××××¨)
        >>> results = search_maamar("", question="××”×™ ×”××”×‘×” ×œ×”×³")
        
        >>> # ×—×™×¤×•×© ×œ×¤×™ ×©× ×” ×‘×œ×‘×“
        >>> results = search_maamar("", year="×ª×©×œ×´×•")
    """
    # 0. ×¢×™×‘×•×“ ×¤×¨××˜×¨×™× - × ×™×§×•×™ ×•×—×™×œ×•×¥ ×©× ×”
    print(f"ğŸ“¥ ×§×œ×˜ ×’×•×œ××™: maamar_name='{maamar_name}', year='{year}', question='{question}'")
    
    # ğŸ†• ×‘×“×•×§ ×× ×™×© ×’×¨×©×™×™× - ×—×™×¤×•×© ××“×•×™×§
    exact_match_only = False
    if maamar_name and maamar_name.strip().startswith('"') and maamar_name.strip().endswith('"'):
        exact_match_only = True
        maamar_name = maamar_name.strip()[1:-1]  # ×”×¡×¨ ××ª ×”×’×¨×©×™×™×
        print(f"ğŸ¯ ×—×™×¤×•×© ××“×•×™×§ (×¢× ×’×¨×©×™×™×): '{maamar_name}'")
    
    # × ×§×” ×©× ××××¨ (×”×¡×¨ "××××¨")
    if maamar_name:
        maamar_name = clean_maamar_name(maamar_name.strip())
        print(f"ğŸ§¹ ××—×¨×™ × ×™×§×•×™ '××××¨': '{maamar_name}'")
    
    # ×—×œ×¥ ×©× ×” ××˜×§×¡×˜ ×× ×œ× × ×ª× ×• ×©× ×” ×‘××¤×•×¨×©
    if maamar_name and not year:
        maamar_name_clean, extracted_year = extract_year_from_text(maamar_name)
        if extracted_year:
            print(f"ğŸ“… ×—×™×œ×¦× ×• ×©× ×” ××”×˜×§×¡×˜: '{extracted_year}'")
            maamar_name = maamar_name_clean
            year = extracted_year
    
    # × ×¨××œ ××ª ×”×©× ×” (×”×¡×¨ ×”×³ ×‘×”×ª×—×œ×” ×•×’×¨×©×™×™×)
    year_normalized = None
    if year:
        year_normalized = year
        year_normalized = re.sub(r"^×”['\×³]?", '', year_normalized)
        year_normalized = re.sub(r'[×´"\'×³/]', '', year_normalized)
        print(f"ğŸ“… ×©× ×” ×× ×•×¨××œ×ª: '{year_normalized}'")
    
    print(f"âœ… ×¤×¨××˜×¨×™× ××¢×•×‘×“×™×: maamar_name='{maamar_name}', year='{year_normalized}', question='{question}'")
    
    # 1. ×˜×¢×Ÿ ××××¨×™× (×¢× cache)
    maamarim = load_maamarim(pkl_source)
    
    # 2. ×‘×“×•×§ ×× ×™×© ×©× ××××¨ ××• ×¨×§ ×©××œ×” ×›×œ×œ×™×ª
    has_maamar_name = bool(maamar_name and maamar_name.strip())
    has_year_only = bool(year_normalized and not has_maamar_name)
    
    # 2.5. ×˜×™×¤×•×œ ×‘×—×™×¤×•×© ×œ×¤×™ ×©× ×” ×‘×œ×‘×“
    if has_year_only:
        print(f"ğŸ“… ×—×™×¤×•×© ×œ×¤×™ ×©× ×” ×‘×œ×‘×“: '{year_normalized}'")
        results = []
        for key, maamar in maamarim.items():
            maamar_year = maamar.get('year', '')
            # × ×¨××œ ××ª ×©× ×ª ×”××××¨
            maamar_year_normalized = re.sub(r"^×”['\×³]?", '', maamar_year)
            maamar_year_normalized = re.sub(r'[×´"\'×³/]', '', maamar_year_normalized)
            
            if maamar_year_normalized == year_normalized:
                results.append({
                    'key': key,
                    'name': maamar.get('name', ''),
                    'year': maamar.get('year', ''),
                    'filename': maamar.get('filename', ''),
                    'text': maamar.get('text', ''),
                    'keywords_all': maamar.get('keywords_all', []),
                    'embedding': maamar.get('embedding'),
                    'score': 100,  # ×›×œ ××××¨ ××”×©× ×” ××§×‘×œ 100
                    'fuzzy_score': 0,
                    'keyword_score': 0,
                    'semantic_score': 0
                })
        
        print(f"ğŸ“Š × ××¦××• {len(results)} ××××¨×™× ××©× ×ª {year}")
        
        # ×× ×™×© ×©××œ×” - ×“×¨×’ ×œ×¤×™ ×ª×•×›×Ÿ
        extracted_keywords = None
        if question and results:
            print(f"ğŸ” ××“×¨×’ {len(results)} ××××¨×™× ×œ×¤×™ ×”×©××œ×”")
            results, extracted_keywords = keyword_search(question, results)
            if use_semantic and any(r.get('embedding') for r in results):
                results = semantic_search(question, results, openai_api_key)

            # ğŸ†• ×× ×™×© ×ª×™×§×• ×‘×¦×™×•×Ÿ - ×“×¨×’ ×œ×¤×™ ×›××•×ª ××–×›×•×¨×™× ×©×œ ××™×œ×•×ª ×”××¤×ª×— ×‘×ª×•×š ×”×˜×§×¡×˜
            tokens = _tokenize_query_keywords(question, extracted_keywords)
            results = _rank_ties_by_keyword_mentions(results, tokens=tokens)
        
        # ×¡× ×Ÿ ×œ×¤×™ ×¦×™×•×Ÿ ××™× ×™××œ×™
        if min_score > 0:
            results = [r for r in results if r['score'] >= min_score]
        
        return results[:max_results]
    
    if has_maamar_name:
        # ××¦×‘ ×¨×’×™×œ: ×™×© ×©× ××××¨
        # ×—×œ×¥ ××ª ×©× ×”××××¨ ×‘×œ×‘×“ (×¢×“ ×”××œ×” ×”×¨××©×•× ×” ×¢× ×’×¨×©×™×™×)
        clean_name = extract_maamar_name_only(maamar_name)
        print(f"ğŸ” extract_maamar_name_only: '{maamar_name}' -> '{clean_name}'")
        
        if not clean_name:
            print(f"âš ï¸  extract_maamar_name_only ×”×—×–×™×¨ ××—×¨×•×–×ª ×¨×™×§×”! ××©×ª××© ×‘-maamar_name ×”××§×•×¨×™")
            clean_name = maamar_name
        
        # 3. Track 1 - ×—×™×¤×•×© ×œ×¤×™ ××¨××” ××§×•×: ××¡×¤×¨ ××™×œ×™× ××“×•×™×§×•×ª ××ª×•×š "××¨××” ×”××§×•×"
        # (××“×•×¨×’ ×œ×¤×™ ××¡×¤×¨ ×”××™×œ×™× ×©× ××¦××•, ×™×•×¨×“)
        # ×× max_results ×”×•× 0 ××• ×©×œ×™×œ×™ - ×§×— ××ª ×›×œ ×”×ª×•×¦××•×ª
        top_n = max_results * 2 if max_results > 0 else 1000
        print(f"ğŸ” ×§×•×¨× ×œ-mara_makom_word_match_search ×¢× clean_name='{clean_name}', top_n={top_n}")
        results = mara_makom_word_match_search(clean_name, maamarim, top_n=top_n)
        print(f"ğŸ“Š mara_makom_word_match_search ×”×—×–×™×¨ {len(results)} ×ª×•×¦××•×ª")
        if results:
            print(f"   ×“×•×’××”: '{results[0].get('name', '')[:50]}'")
        
        # 3.5. ×¡×™× ×•×Ÿ ×œ×¤×™ ×©× ×” ×× ×¦×•×™×Ÿ
        if year_normalized and results:
            print(f"ğŸ“… ××¡× ×Ÿ ×œ×¤×™ ×©× ×”: '{year_normalized}'")
            filtered_results = []
            for r in results:
                maamar_year = r.get('year') or ''
                if not maamar_year:
                    # Fallback: × ×¡×” ×œ×—×œ×¥ ××”×©× ×× ××™×Ÿ ×©×“×” year
                    name = r.get('name', '')
                    year_match = re.search(r'×ª×©[×-×ª]{1,2}(?:[×´"×³\'][×-×ª]|[×-×ª])?', name)
                    if year_match:
                        maamar_year = year_match.group(0)
                    else:
                        continue  # ××™×Ÿ ×©× ×” - ××“×œ×’
                
                maamar_year_normalized = re.sub(r"^×”['\×³]?", '', maamar_year)
                maamar_year_normalized = re.sub(r'[×´"\'×³/]', '', maamar_year_normalized)
                
                if maamar_year_normalized == year_normalized:
                    filtered_results.append(r)
            
            print(f"   × ××¦××• {len(filtered_results)} ××ª×•×š {len(results)} ××××¨×™× ×¢× ×”×©× ×” ×”××‘×•×§×©×ª")
            results = filtered_results
        
        # ğŸ¯ Track 1 rule (requested):
        # - Primary ranking MUST be only by mareh-makom word overlap with the maamar title (words_found desc).
        # - Only if multiple results share the SAME words_found, break ties using the question keywords.
        # - If match is perfect (all mareh-makom words found) -> return ONLY 1.
        # - Otherwise -> return up to 3.

        if not results:
            print("âŒ ×œ× × ××¦××• ××××¨×™× ×œ×¤×™ ××¨××” ××§×•×")
            return []

        # group by words_found (desc)
        max_words_found = max(int(r.get('words_found') or 0) for r in results) if results else 0
        total_words = int(results[0].get('total_words') or 0) if results else 0
        is_perfect = bool(total_words and max_words_found >= total_words)

        # ğŸ†• ×× ×™×© ×©××œ×” - ××™×™×Ÿ ×œ×¤×™ ××¡×¤×¨ ××™×œ×•×ª ×”××¤×ª×— ×‘×ª×•×š ×›×œ ×§×‘×•×¦×” ×©×œ words_found
        if question and results:
            print(f"ğŸ” ×××™×™×Ÿ ×œ×¤×™ ××™×œ×•×ª ××¤×ª×— ×‘×ª×•×š ×§×‘×•×¦×•×ª words_found")
            # ×—×œ×¥ ××™×œ×•×ª ××¤×ª×— ××”×©××œ×”
            extracted_keywords = extract_keywords_from_question(question)
            if extracted_keywords:
                print(f"   ××™×œ×•×ª ××¤×ª×—: {extracted_keywords}")
                # ×—×©×‘ ××¡×¤×¨ ××™×œ×•×ª ××¤×ª×— ×©× ××¦××• ×‘×›×œ ××××¨ (count > 0)
                for r in results:
                    text = r.get('text', '')
                    # ×”×©×ª××© ×‘-_count_phrase_mentions_in_text ×›×“×™ ×œ×¡×¤×•×¨ ××•×¤×¢×™×
                    keyword_counts = _count_phrase_mentions_in_text(text, extracted_keywords)
                    # ×¡×¤×•×¨ ×›××” ××™×œ×•×ª ××¤×ª×— × ××¦××• (count > 0)
                    keywords_found_count = sum(1 for kw in extracted_keywords if int(keyword_counts.get(kw, 0) or 0) > 0)
                    # ×©××•×¨ ××ª ××¡×¤×¨ ××™×œ×•×ª ×”××¤×ª×— ×©× ××¦××• ×•××ª ×”×¡×›×•× ×”×›×•×œ×œ
                    r['_matched_keywords_count'] = keywords_found_count
                    r['_matched_keywords_total'] = sum(int(keyword_counts.get(kw, 0) or 0) for kw in extracted_keywords)
                    r['_matched_keyword_counts'] = keyword_counts
                    # ğŸ†• ×—×©×‘ ×”×× ×™×© ××ª ×›×œ ××™×œ×•×ª ×”××¤×ª×— (×›×œ count > 0)
                    has_all_keywords = keywords_found_count == len(extracted_keywords)
                    r['_has_all_keywords'] = has_all_keywords
                    # ×œ×•×’ ×œ×“×™×‘×•×’
                    if not has_all_keywords:
                        missing = [kw for kw in extracted_keywords if int(keyword_counts.get(kw, 0) or 0) == 0]
                        print(f"      âš ï¸ '{r.get('name', '')[:40]}' - ×—×¡×¨×•×ª ××™×œ×•×ª: {missing}, counts={keyword_counts}")
                
                # ğŸ†• ××™×™×Ÿ ×œ×¤×™:
                # 1. ×¨××©×™ - ×œ×¤×™ ××—×•×–×™× (words_found/total_words * 100) - ×™×•×¨×“
                # 2. ×‘×ª×•×š ×›×œ ×¦×™×•×Ÿ - ×”×× ×™×© ××ª ×›×œ ××™×œ×•×ª ×”××¤×ª×— (True ×œ×¤× ×™ False), ×•××– ×¡×”"×› ×”××•×¤×¢×™× (×™×•×¨×“)
                def sort_key(r):
                    words_found = int(r.get('words_found', 0) or 0)
                    total_words = int(r.get('total_words', 0) or 0)
                    # ×—×©×‘ ××—×•×–×™× (×× total_words > 0)
                    if total_words > 0:
                        words_percentage = (words_found / total_words) * 100
                    else:
                        words_percentage = 0 if words_found == 0 else 100
                    
                    # ×‘×ª×•×š ×›×œ ×¦×™×•×Ÿ - ××™×•×Ÿ ×¤× ×™××™
                    has_all = r.get('_has_all_keywords', False)
                    total_mentions = int(r.get('_matched_keywords_total', 0) or 0)
                    
                    # ×”×—×–×¨ tuple ×œ××™×•×Ÿ (×¢× reverse=True):
                    # 1. ××—×•×–×™× (×™×•×¨×“) - ×™×•×ª×¨ ×’×“×•×œ = ×¨××©×•×Ÿ
                    # 2. ×”×× ×™×© ××ª ×›×œ ×”××™×œ×•×ª (True ×œ×¤× ×™ False) - has_all ×™×©×™×¨×•×ª, reverse=True ×™×‘×™× True ×œ×¤× ×™ False
                    # 3. ×¡×”"×› ×”××•×¤×¢×™× (×™×•×¨×“) - ×™×•×ª×¨ ×’×“×•×œ = ×¨××©×•×Ÿ
                    return (
                        words_percentage,  # ×™×•×¨×“ (×™×•×ª×¨ ××—×•×–×™× = ×¨××©×•×Ÿ)
                        has_all,  # True ×œ×¤× ×™ False (×¢× reverse=True, True ×™×‘×•× ×œ×¤× ×™ False - × ×›×•×Ÿ!)
                        total_mentions  # ×™×•×¨×“ (×™×•×ª×¨ ××•×¤×¢×™× = ×¨××©×•×Ÿ)
                    )
                
                results.sort(key=sort_key, reverse=True)
                print(f"   ğŸ“Š ××—×¨×™ ××™×•×Ÿ ×œ×¤×™ ××™×œ×•×ª ××¤×ª×—:")
                for i, r in enumerate(results[:5], 1):
                    print(f"      {i}. '{r.get('name', '')[:40]}' - words_found={r.get('words_found', 0)}, keywords_found={r.get('_matched_keywords_count', 0)}/{len(extracted_keywords)}, total={r.get('_matched_keywords_total', 0)}")
            else:
                # ××™×Ÿ ××™×œ×•×ª ××¤×ª×— - ×¨×§ ××™×•×Ÿ ×œ×¤×™ words_found
                results.sort(key=lambda r: int(r.get('words_found', 0)), reverse=True)

        # ×× ×™×© ×”×ª×××” ××•×©×œ××ª - ×”×—×–×¨ ××ª ×›×œ ×”××××¨×™× ×¢× ×”×ª×××” ××•×©×œ××ª (×¢×“ max_results)
        if is_perfect:
            # ××¦× ××ª ×›×œ ×”××××¨×™× ×¢× words_found == total_words (×”×ª×××” ××•×©×œ××ª)
            perfect_results = [r for r in results if int(r.get('words_found', 0)) >= total_words]
            print(f"âœ¨ ×”×ª×××” ××•×©×œ××ª ×‘××¨××” ××§×•× - × ××¦××• {len(perfect_results)} ××××¨×™× ×¢× ×”×ª×××” ××•×©×œ××ª")
            # ×× max_results ×”×•× 0 ××• ×©×œ×™×œ×™ - ×”×—×–×¨ ××ª ×›×œ ×”××××¨×™× ×”××•×©×œ××™×
            if max_results <= 0:
                return perfect_results
            return perfect_results[:max_results]
        # ×× ××™×Ÿ ×”×ª×××” ××•×©×œ××ª - ×”×—×–×¨ ×¢×“ 3 (××• max_results ×× ×”×•× ×’×“×•×œ ×-0)
        limit = max_results if max_results > 0 else 3
        return results[:limit]
    
    else:
        # ××¦×‘ ×©××œ×” ×›×œ×œ×™×ª: ××™×Ÿ ×©× ××××¨ - ×¨×§ ×©××œ×”!
        print("ğŸ” ×©××œ×” ×›×œ×œ×™×ª (×œ×œ× ×©× ××××¨) - ××—×¤×© ×‘×ª×•×›×Ÿ ×‘×œ×‘×“")
        
        if not question:
            print("âš ï¸  ××™×Ÿ ×©× ××××¨ ×•××™×Ÿ ×©××œ×” - ×œ× × ×™×ª×Ÿ ×œ×—×¤×©")
            return []
        
        # ×”×—×–×¨ ××ª ×›×œ ×”××××¨×™× ×¢× ×¦×™×•×Ÿ ×¨××©×•× ×™ ×©×•×•×”
        results = []
        for key, maamar in maamarim.items():
            results.append({
                'key': key,
                'name': maamar.get('name', ''),
                'filename': maamar.get('filename', ''),
                'text': maamar.get('text', ''),
                'keywords_all': maamar.get('keywords_all', []),
                'embedding': maamar.get('embedding'),
                'score': 50,  # ×¦×™×•×Ÿ ×¨××©×•× ×™ ×©×•×•×” ×œ×›×•×œ×
                'fuzzy_score': 0,  # ××™×Ÿ ×—×™×¤×•×© fuzzy
                'keyword_score': 0,
                'semantic_score': 0
            })
        
        # ×“×¨×’ ×œ×¤×™ ××™×œ×•×ª ××¤×ª×— (Grok)
        results, extracted_keywords = keyword_search(question, results)
        
        # ×“×¨×•×’ ×¡×× ×˜×™ (OpenAI)
        if use_semantic and any(r.get('embedding') for r in results):
            results = semantic_search(question, results, openai_api_key)

        # ğŸ†• ×× ×™×© ×ª×™×§×• ×‘×¦×™×•×Ÿ - ×“×¨×’ ×œ×¤×™ ×›××•×ª ××–×›×•×¨×™× ×©×œ ××™×œ×•×ª ×”××¤×ª×— ×‘×ª×•×š ×”×˜×§×¡×˜
        tokens = _tokenize_query_keywords(question, extracted_keywords)
        results = _rank_ties_by_keyword_mentions(results, tokens=tokens)
    
    # 5. ×”×—×–×¨ ××ª max_results ×”×˜×•×‘×™× ×‘×™×•×ª×¨ (×œ×œ× ×¡×£ ××™× ×™××œ×™!)
    # ×ª××™×“ ××—×–×™×¨×™× ××ª ×”×˜×•×‘×™× ×‘×™×•×ª×¨, ×’× ×× ×”×¦×™×•×Ÿ × ××•×š
    return results[:max_results]


def search_and_print(maamar_name: str, question: Optional[str] = None):
    """
    ×—×™×¤×•×© ×•×”×“×¤×¡×” (×œ×‘×“×™×§×•×ª)
    """
    print("="*70)
    print(f"ğŸ” ××—×¤×©: {maamar_name}")
    if question:
        print(f"â“ ×©××œ×”: {question}")
    print("="*70)
    
    try:
        results = search_maamar(maamar_name, question)
        
        if not results:
            print("âŒ ×œ× × ××¦××• ××××¨×™×")
            return
        
        print(f"\nâœ… × ××¦××• {len(results)} ××××¨×™×:\n")
        
        for i, result in enumerate(results, 1):
            print(f"{i}. {result['name']}")
            print(f"   ğŸ“Š ×¦×™×•×Ÿ: {result['score']}%")
            print(f"   ğŸ“„ ×§×•×‘×¥: {result['filename']}")
            print(f"   ğŸ“ ×˜×§×¡×˜: {len(result['text'])} ×ª×•×•×™×")
            if i == 1:  # ×”×¦×’ ×§×¦×ª ××”×˜×§×¡×˜ ×©×œ ×”×ª×•×¦××” ×”×¨××©×•× ×”
                preview = result['text'][:150] + "..." if len(result['text']) > 150 else result['text']
                print(f"   ğŸ“– ×ª×¦×•×’×” ××§×“×™××”: {preview}")
            print()
    
    except Exception as e:
        print(f"âŒ ×©×’×™××”: {e}")
        import traceback
        traceback.print_exc()


# ========== MAIN (×œ×‘×“×™×§×•×ª) ==========
if __name__ == "__main__":
    print("\n" + "="*70)
    print("ğŸ§ª ×‘×“×™×§×•×ª ×—×™×¤×•×© ××××¨×™×")
    print("="*70 + "\n")
    
    # ×‘×“×™×§×” 1: ×—×™×¤×•×© ×¤×©×•×˜
    search_and_print("×•××‘×¨×”× ×–×§×Ÿ")
    
    # ×‘×“×™×§×” 2: ×—×™×¤×•×© ×¢× ×©× ×”
    search_and_print("×‘××ª×™ ×œ×’× ×™ ×ª×©×™×")
    
    # ×‘×“×™×§×” 3: ×¢× ×©××œ×”
    search_and_print("×•××‘×¨×”× ×–×§×Ÿ", "×§×™×•× ×”××¦×•×•×ª")
    
    print("\n" + "="*70)
    print("âœ… ×‘×“×™×§×•×ª ×”×¡×ª×™×™××•!")
    print("="*70)
